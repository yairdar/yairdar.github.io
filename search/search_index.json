{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Aguide \u00b6 A guide to Sowftware Gardening This site has technical informaition about yairdar projects About Yair Dar Dev \u00b6 Personal Site yairdar.com I have 10 years hands on developer exeprience. In different roles: developer, fullstack, devops, architect. My main areas of experience are DevOps, CI/CD, Delivery and Automation Projects \u00b6 Technical Documentation about projects yairdar.github.io aguide Technical Articles and Guides devpod Os Blocks for Image Builds and Setups jenlib Jenkins Automation on Steroids tasker Taskfile Visualisations and packs Essential Tooling \u00b6 task or taskfile.dev of Taskfile tool, is corner stone of robust automation. checkout Taskfile Tool rclone or rclone site of rclone tool, is corner stone of storage (mount) mapping and data transfer. checkout Rclone Tool yq or yq site of yq tool, is corner stone of query configs and data from files, including k8s files, taskfiles etc. checkout yq Tool Contacts \u00b6 Contacts LinkedIn Personal Web Site GitHub YairDar","title":"Home"},{"location":"#aguide","text":"A guide to Sowftware Gardening This site has technical informaition about yairdar projects","title":"Aguide"},{"location":"#about-yair-dar-dev","text":"Personal Site yairdar.com I have 10 years hands on developer exeprience. In different roles: developer, fullstack, devops, architect. My main areas of experience are DevOps, CI/CD, Delivery and Automation","title":"About Yair Dar Dev"},{"location":"#projects","text":"Technical Documentation about projects yairdar.github.io aguide Technical Articles and Guides devpod Os Blocks for Image Builds and Setups jenlib Jenkins Automation on Steroids tasker Taskfile Visualisations and packs","title":"Projects"},{"location":"#essential-tooling","text":"task or taskfile.dev of Taskfile tool, is corner stone of robust automation. checkout Taskfile Tool rclone or rclone site of rclone tool, is corner stone of storage (mount) mapping and data transfer. checkout Rclone Tool yq or yq site of yq tool, is corner stone of query configs and data from files, including k8s files, taskfiles etc. checkout yq Tool","title":"Essential Tooling"},{"location":"#contacts","text":"Contacts LinkedIn Personal Web Site GitHub YairDar","title":"Contacts"},{"location":"base-tools/rclone/","text":"Rclone \u00b6 Official Site About rclone \u00b6 Rclone is a command-line program to manage files on cloud storage. It is a feature-rich alternative to cloud vendors' web storage interfaces. Over 40 cloud storage products support rclone including S3 object stores, business & consumer file storage services, as well as standard transfer protocols. Rclone has powerful cloud equivalents to the unix commands rsync, cp, mv, mount, ls, ncdu, tree, rm, and cat. Rclone's familiar syntax includes shell pipeline support, and --dry-run protection. It is used at the command line, in scripts or via its API. Users call rclone \"The Swiss army knife of cloud storage\", and \"Technology indistinguishable from magic\".","title":"Rclone"},{"location":"base-tools/rclone/#rclone","text":"Official Site","title":"Rclone"},{"location":"base-tools/rclone/#about-rclone","text":"Rclone is a command-line program to manage files on cloud storage. It is a feature-rich alternative to cloud vendors' web storage interfaces. Over 40 cloud storage products support rclone including S3 object stores, business & consumer file storage services, as well as standard transfer protocols. Rclone has powerful cloud equivalents to the unix commands rsync, cp, mv, mount, ls, ncdu, tree, rm, and cat. Rclone's familiar syntax includes shell pipeline support, and --dry-run protection. It is used at the command line, in scripts or via its API. Users call rclone \"The Swiss army knife of cloud storage\", and \"Technology indistinguishable from magic\".","title":"About rclone"},{"location":"base-tools/taskfile/","text":"Taskfile \u00b6 Taskfile for Development Automation \u00b6 What\u2019s the point of Taskfile? Human-friendly automation with a smooth learning curve and serious abilities. Offical Site Generic Automation Layer \u00b6 Taskfile provides unified API for devops needs Heterogeneous projects \u00b6 Heterogeneous projects are projects, where part of code written in python / cpp / java / javascript / typescript we have a zoo of \u201chigh level tasks definitions\u201d Maven, conda, pip, pdm, bower, npm, gradle So high level language that connects them all became a jenkins groovy file, or github actions (which is not useful for complex integration tests or MLops needs) Project Agnostic Commands \u00b6 With taskfile based automations it doesn't matter what language or build system is used, we can always expect the next behavior from project task build task test task publish And for integration projects task fetch-deps task run-integration-tests task deploy task validate It allows building cross-project tooling without relaying or treating specific use cases for specific build systems, since all the differences are taken care of on the taskfile level. Key advantages of Taskfile \u00b6 Human readable, machine executable \u00b6 The advantages of YAML for automation Like Makefile or bash script, but human readable. Like Markdown docs, but executable. From simplicity to complexity \u00b6 Taskfile allows you to make small easy steps first and to achieve more complex behavior by chaining them afterwards. Task names that make sense \u00b6 Thanks to YAML syntax and description field for each task, it\u2019s easy to track what you\u2019re doing even in complex automations. Just make sure that you give tasks appropriate names and provide explicit descriptions. Taskfile works everywhere \u00b6 A lightweight utility that helps a lot You can use Taskfile on local machines, servers, Docker containers, CI CD systems like Jenkins or Github Actions. Most of the time it\u2019s fairly easy to adapt your Taskfiles for a different platform. Usually you can use them as they are without even editing them, or just adding platform-specific tasks to existing ones. Easily Parsable by other tools \u00b6 Since tasks are written in YAML, they could be parsed in any language, or even in bash console using tools like yq. This opens a new horizons to automation tools (like jenlib) This could be used for task security validation by enforcing of some rules for different tasks types The very thing that a task is either small bash code blocks or composition of other tasks the verification of such block based code is much simpler than analysis of Makefiles, python scripts, bash scripts or groovy scripts. IDE support \u00b6 vscode idea See more community tools at https://taskfile.dev/community/ Shell completions \u00b6 See go-task-completions Features \u00b6 Separation of build system from automation DAG. Efficient and simple parallelism Getting started \u00b6 install tutorial official docs","title":"Taskfile"},{"location":"base-tools/taskfile/#taskfile","text":"","title":"Taskfile"},{"location":"base-tools/taskfile/#taskfile-for-development-automation","text":"What\u2019s the point of Taskfile? Human-friendly automation with a smooth learning curve and serious abilities. Offical Site","title":"Taskfile for Development Automation"},{"location":"base-tools/taskfile/#generic-automation-layer","text":"Taskfile provides unified API for devops needs","title":"Generic Automation Layer"},{"location":"base-tools/taskfile/#heterogeneous-projects","text":"Heterogeneous projects are projects, where part of code written in python / cpp / java / javascript / typescript we have a zoo of \u201chigh level tasks definitions\u201d Maven, conda, pip, pdm, bower, npm, gradle So high level language that connects them all became a jenkins groovy file, or github actions (which is not useful for complex integration tests or MLops needs)","title":"Heterogeneous projects"},{"location":"base-tools/taskfile/#project-agnostic-commands","text":"With taskfile based automations it doesn't matter what language or build system is used, we can always expect the next behavior from project task build task test task publish And for integration projects task fetch-deps task run-integration-tests task deploy task validate It allows building cross-project tooling without relaying or treating specific use cases for specific build systems, since all the differences are taken care of on the taskfile level.","title":"Project Agnostic Commands"},{"location":"base-tools/taskfile/#key-advantages-of-taskfile","text":"","title":"Key advantages of Taskfile"},{"location":"base-tools/taskfile/#human-readable-machine-executable","text":"The advantages of YAML for automation Like Makefile or bash script, but human readable. Like Markdown docs, but executable.","title":"Human readable, machine executable"},{"location":"base-tools/taskfile/#from-simplicity-to-complexity","text":"Taskfile allows you to make small easy steps first and to achieve more complex behavior by chaining them afterwards.","title":"From simplicity to complexity"},{"location":"base-tools/taskfile/#task-names-that-make-sense","text":"Thanks to YAML syntax and description field for each task, it\u2019s easy to track what you\u2019re doing even in complex automations. Just make sure that you give tasks appropriate names and provide explicit descriptions.","title":"Task names that make sense"},{"location":"base-tools/taskfile/#taskfile-works-everywhere","text":"A lightweight utility that helps a lot You can use Taskfile on local machines, servers, Docker containers, CI CD systems like Jenkins or Github Actions. Most of the time it\u2019s fairly easy to adapt your Taskfiles for a different platform. Usually you can use them as they are without even editing them, or just adding platform-specific tasks to existing ones.","title":"Taskfile works everywhere"},{"location":"base-tools/taskfile/#easily-parsable-by-other-tools","text":"Since tasks are written in YAML, they could be parsed in any language, or even in bash console using tools like yq. This opens a new horizons to automation tools (like jenlib) This could be used for task security validation by enforcing of some rules for different tasks types The very thing that a task is either small bash code blocks or composition of other tasks the verification of such block based code is much simpler than analysis of Makefiles, python scripts, bash scripts or groovy scripts.","title":"Easily Parsable by other tools"},{"location":"base-tools/taskfile/#ide-support","text":"vscode idea See more community tools at https://taskfile.dev/community/","title":"IDE support"},{"location":"base-tools/taskfile/#shell-completions","text":"See go-task-completions","title":"Shell completions"},{"location":"base-tools/taskfile/#features","text":"Separation of build system from automation DAG. Efficient and simple parallelism","title":"Features"},{"location":"base-tools/taskfile/#getting-started","text":"install tutorial official docs","title":"Getting started"},{"location":"base-tools/yq/","text":"YQ \u00b6 docs source yq is a lightweight and portable command-line YAML, JSON and XML processor. yq uses jq like syntax but works with yaml files as well as json, xml, properties, csv and tsv. It doesn't yet support everything jq does - but it does support the most common operations and functions, and more is being added continuously.","title":"YQ"},{"location":"base-tools/yq/#yq","text":"docs source yq is a lightweight and portable command-line YAML, JSON and XML processor. yq uses jq like syntax but works with yaml files as well as json, xml, properties, csv and tsv. It doesn't yet support everything jq does - but it does support the most common operations and functions, and more is being added continuously.","title":"YQ"},{"location":"base-tutorials/a-tasker/","text":"Intro Tutorial \u00b6 This tutorial contains 8 chapters with practical exsercises to master Taskfile from basics to more realistic task in the final chapter. Getting started Task Directory Including Taskfiles Variables Environment Variables Dependencies - Run Tasks in Parallel Passing CLI Arguments to the Task Requests - an Applied Taskfile Example","title":"Intro Tutorial"},{"location":"base-tutorials/a-tasker/#intro-tutorial","text":"This tutorial contains 8 chapters with practical exsercises to master Taskfile from basics to more realistic task in the final chapter. Getting started Task Directory Including Taskfiles Variables Environment Variables Dependencies - Run Tasks in Parallel Passing CLI Arguments to the Task Requests - an Applied Taskfile Example","title":"Intro Tutorial"},{"location":"base-tutorials/a-tasker/c01_getting_started/","text":"Getting started \u00b6 Getting Started and How to Call One Task from Another in official documentation Getting started \u00b6 Here is the Taskfile to follow along. Open the working directory in terminal (I suggest you to use ./working_dir ) First install Taskfile according to the instructions from the official site. I use the Install Script and it works just fine. To create a new taskfile in the directory run a command: task --init A new file, Taskfile.yaml, will appear. Open this file in any text editor. Its content will look like this: # https://taskfile.dev version : '3' vars : GREETING : Hello, World! tasks : default : cmds : - echo \"{{.GREETING}}\" silent : true If you'll run a command task with no arguments, Taskfile will run the deafult task, hence it's name. To make your own task, type it below the exisiting one. Mind indentations, so that task name should be indented the same amount as the default task name and so on. The result should look like this: # https://taskfile.dev version : '3' vars : GREETING : Hello, World! tasks : default : cmds : - echo \"{{.GREETING}}\" silent : true first-task : cmds : - echo \"My first task runs fine\" Now try running task first-task . It should execute the command that you have written in cmds section. By the way: You can print out the list of all tasks with task -a command. It's a good practice to add this command to the default task, so you can call it simply typing task . tasks : default : cmds : - echo \"{{.GREETING}}\" - task -a silent : true So far so good. Now let's add the description to our task. First it's a good practice to know what the task's doing. Also, even if the task is self-explanatory, a description allows Taskfile shell completion to work. For simple tasks like this just add a dummy desc: _ line: first-task : desc : _ cmds : - echo \"My first task runs fine\" Now let's add more tasks. First of them will be almost exactly the same as before: get-started : desc : _ cmds : - echo \"Call any task to get started\" The next task is a little bit special: it calls another two tasks. Note that calling tasks goes like this: - task: taskname in cmds section. calling-tasks : desc : one task can call another tasks, like this one does cmds : - task : get-started - task : first-task Try running this task: task calling-tasks It should call two other tasks that you have written before. Voila! Now we can chain tasks and combine them to fit our needs. Note: You can use shell commands in task to call another task, like this: - task taskname but it's not recommended . It will work in simple situations when all the tasks are in one Taskfile in your working directory and no environment variables are involved. But in more complex setups it may lead to confusion, so please consider using the native task: taskname method. Even with this basic toolset Taskfile can make life much easier. But there's much more to it, as you'll see. Next chapter","title":"Getting started"},{"location":"base-tutorials/a-tasker/c01_getting_started/#getting-started","text":"Getting Started and How to Call One Task from Another in official documentation","title":"Getting started"},{"location":"base-tutorials/a-tasker/c01_getting_started/#getting-started_1","text":"Here is the Taskfile to follow along. Open the working directory in terminal (I suggest you to use ./working_dir ) First install Taskfile according to the instructions from the official site. I use the Install Script and it works just fine. To create a new taskfile in the directory run a command: task --init A new file, Taskfile.yaml, will appear. Open this file in any text editor. Its content will look like this: # https://taskfile.dev version : '3' vars : GREETING : Hello, World! tasks : default : cmds : - echo \"{{.GREETING}}\" silent : true If you'll run a command task with no arguments, Taskfile will run the deafult task, hence it's name. To make your own task, type it below the exisiting one. Mind indentations, so that task name should be indented the same amount as the default task name and so on. The result should look like this: # https://taskfile.dev version : '3' vars : GREETING : Hello, World! tasks : default : cmds : - echo \"{{.GREETING}}\" silent : true first-task : cmds : - echo \"My first task runs fine\" Now try running task first-task . It should execute the command that you have written in cmds section. By the way: You can print out the list of all tasks with task -a command. It's a good practice to add this command to the default task, so you can call it simply typing task . tasks : default : cmds : - echo \"{{.GREETING}}\" - task -a silent : true So far so good. Now let's add the description to our task. First it's a good practice to know what the task's doing. Also, even if the task is self-explanatory, a description allows Taskfile shell completion to work. For simple tasks like this just add a dummy desc: _ line: first-task : desc : _ cmds : - echo \"My first task runs fine\" Now let's add more tasks. First of them will be almost exactly the same as before: get-started : desc : _ cmds : - echo \"Call any task to get started\" The next task is a little bit special: it calls another two tasks. Note that calling tasks goes like this: - task: taskname in cmds section. calling-tasks : desc : one task can call another tasks, like this one does cmds : - task : get-started - task : first-task Try running this task: task calling-tasks It should call two other tasks that you have written before. Voila! Now we can chain tasks and combine them to fit our needs. Note: You can use shell commands in task to call another task, like this: - task taskname but it's not recommended . It will work in simple situations when all the tasks are in one Taskfile in your working directory and no environment variables are involved. But in more complex setups it may lead to confusion, so please consider using the native task: taskname method. Even with this basic toolset Taskfile can make life much easier. But there's much more to it, as you'll see. Next chapter","title":"Getting started"},{"location":"base-tutorials/a-tasker/c02_task_directory/","text":"Task Directory \u00b6 Task Directory in official documentation Task Directory \u00b6 Here is the Taskfile to follow along. Open the working directory in terminal (I suggest you to use ./working_dir ) By default the task runs in the same dircetory where the Taskfile is located, but for any task you can specify the dircetory where to run it: dir: path Let's create an empty taskfile in our working directory ( task --init ) and add a new task to it. new-text-file : desc : make a new text file inside a new directory dir : temp cmds : - echo \"That's how task directories work\" > thats_how.txt Try to run the new task: task new-text-file . Note that the subdirectory gets created if it doesn't yet exist. The next task will output the content of the text file to the terminal. This task should also work in the same subdirectory. read-text : desc : read the text file from a subdirectory dir : temp cmds : - cat thats_how.txt Add it to the Taskfile and try it out: task read-text After you have tried tasks with specified working directories it's time to clean up our workshop. Let's create a task for this. This time will work in the same directory where the Taskfile is, so there's no need to specify the directory. cleanup : desc : remove a subdirectory so you can start from scratch cmds : - rm -r temp Now you can run task cleanup and start over any time you want. Next chapter","title":"Task Directory"},{"location":"base-tutorials/a-tasker/c02_task_directory/#task-directory","text":"Task Directory in official documentation","title":"Task Directory"},{"location":"base-tutorials/a-tasker/c02_task_directory/#task-directory_1","text":"Here is the Taskfile to follow along. Open the working directory in terminal (I suggest you to use ./working_dir ) By default the task runs in the same dircetory where the Taskfile is located, but for any task you can specify the dircetory where to run it: dir: path Let's create an empty taskfile in our working directory ( task --init ) and add a new task to it. new-text-file : desc : make a new text file inside a new directory dir : temp cmds : - echo \"That's how task directories work\" > thats_how.txt Try to run the new task: task new-text-file . Note that the subdirectory gets created if it doesn't yet exist. The next task will output the content of the text file to the terminal. This task should also work in the same subdirectory. read-text : desc : read the text file from a subdirectory dir : temp cmds : - cat thats_how.txt Add it to the Taskfile and try it out: task read-text After you have tried tasks with specified working directories it's time to clean up our workshop. Let's create a task for this. This time will work in the same directory where the Taskfile is, so there's no need to specify the directory. cleanup : desc : remove a subdirectory so you can start from scratch cmds : - rm -r temp Now you can run task cleanup and start over any time you want. Next chapter","title":"Task Directory"},{"location":"base-tutorials/a-tasker/c03_including_taskfiles/","text":"Including Taskfiles \u00b6 Including other Taskfiles in official documentation Intro to Including Taskfiles \u00b6 Here is the Taskfile to follow along. Open the working directory in terminal (I suggest you to use ./working_dir ) Until now we only tried to run the tasks that are in the Taskfile.yaml in our working directory. In many cases it's enough, but there is an option to include external taskfiles to the main one. Let's do it! Create a new taskfile in the working directory: task --init . Don't add anything to this file yet. Make a new file: touch greet.tasks.yaml Now let's make a task in our external file. Open greet.tasks.yaml in a text editor that you prefer and add the content: # https://taskfile.dev version : '3' tasks : simple_greet : desc : 'Call this task like this: task from-another-file:simple_greet' cmds : - echo \"Hi from included file\" This is a whole valid taskfile that we will include to our main Taskfile.yaml. To do it, let's add to our taskfile (after version: '3' line): includes : from-another-file : taskfile : greet.tasks.yaml Try to call a task from the included file: task from-another-file:simple-greet If it writes out Hi from included file , then we're good to go. Note: It's possible to run tasks from taskfiles that aren't named Taskfile.yaml. To do this, use task -t taskfile-name.yaml taskname command. For instance, try task -t greet.tasks.yaml simple-greet Including directories \u00b6 We can include not only files, but directories. In this case the Taskfile.yaml from this directory gets included so we access its tasks from our main taskfile. Let's include the directory. When To do this, create a new directory: mkdir greet-dir Go to this directroy ( cd greet-dir ) and create a new taskfile ( task --init ). Open this new Taskfile.yaml with a text editor and overwrite its content with following block: # https://taskfile.dev version : '3' tasks : simple-greet : desc : 'Call this task like this: task from-subdir:simple_greet' cmds : - echo \"Hello from a subdirectory!\" Now go back to the worknig directory ( cd .. ) and open the main Taskfile.yaml in the text editor. Add the following line to the includes: block: from-subdir: ./greet_dir The result should look like this: includes : from-another-file : taskfile : greet.tasks.yaml from-subdir : ./greet_dir Save the file and try to run: task from-subdir:simple-greet If it writes out Hello from a subdirectory! , then it works as expected. And that's basically it, except for... Optional includes \u00b6 By default it the included taskfile is not found the main taskfile will not work. If you add optional: true line to the include, it's not the case any more. The main taskfile will work just fine even if this particular include is not found. Here's the example: includes : from-another-file : taskfile : greet.tasks.yaml from-subdir : ./greet_dir from-nonexisting-file : taskfile : taskfile.thats.not.there.yaml optional : true Note that from-nonexisting-file include is optional, so the taskfile works fine even if the file's not there. Now you know how to include external Taskfiles. Note: You can't chain includes , like including the taskfile that already includes other taskfiles. You can walkaround this limitation calling tasks from another taskfiles with commands, like task -t taskfile-name.yaml taskname , that's where this syntax comes in handy. But whenever you can avoid complexity, please do. Next chapter","title":"Including Taskfiles"},{"location":"base-tutorials/a-tasker/c03_including_taskfiles/#including-taskfiles","text":"Including other Taskfiles in official documentation","title":"Including Taskfiles"},{"location":"base-tutorials/a-tasker/c03_including_taskfiles/#intro-to-including-taskfiles","text":"Here is the Taskfile to follow along. Open the working directory in terminal (I suggest you to use ./working_dir ) Until now we only tried to run the tasks that are in the Taskfile.yaml in our working directory. In many cases it's enough, but there is an option to include external taskfiles to the main one. Let's do it! Create a new taskfile in the working directory: task --init . Don't add anything to this file yet. Make a new file: touch greet.tasks.yaml Now let's make a task in our external file. Open greet.tasks.yaml in a text editor that you prefer and add the content: # https://taskfile.dev version : '3' tasks : simple_greet : desc : 'Call this task like this: task from-another-file:simple_greet' cmds : - echo \"Hi from included file\" This is a whole valid taskfile that we will include to our main Taskfile.yaml. To do it, let's add to our taskfile (after version: '3' line): includes : from-another-file : taskfile : greet.tasks.yaml Try to call a task from the included file: task from-another-file:simple-greet If it writes out Hi from included file , then we're good to go. Note: It's possible to run tasks from taskfiles that aren't named Taskfile.yaml. To do this, use task -t taskfile-name.yaml taskname command. For instance, try task -t greet.tasks.yaml simple-greet","title":"Intro to Including Taskfiles"},{"location":"base-tutorials/a-tasker/c03_including_taskfiles/#including-directories","text":"We can include not only files, but directories. In this case the Taskfile.yaml from this directory gets included so we access its tasks from our main taskfile. Let's include the directory. When To do this, create a new directory: mkdir greet-dir Go to this directroy ( cd greet-dir ) and create a new taskfile ( task --init ). Open this new Taskfile.yaml with a text editor and overwrite its content with following block: # https://taskfile.dev version : '3' tasks : simple-greet : desc : 'Call this task like this: task from-subdir:simple_greet' cmds : - echo \"Hello from a subdirectory!\" Now go back to the worknig directory ( cd .. ) and open the main Taskfile.yaml in the text editor. Add the following line to the includes: block: from-subdir: ./greet_dir The result should look like this: includes : from-another-file : taskfile : greet.tasks.yaml from-subdir : ./greet_dir Save the file and try to run: task from-subdir:simple-greet If it writes out Hello from a subdirectory! , then it works as expected. And that's basically it, except for...","title":"Including directories"},{"location":"base-tutorials/a-tasker/c03_including_taskfiles/#optional-includes","text":"By default it the included taskfile is not found the main taskfile will not work. If you add optional: true line to the include, it's not the case any more. The main taskfile will work just fine even if this particular include is not found. Here's the example: includes : from-another-file : taskfile : greet.tasks.yaml from-subdir : ./greet_dir from-nonexisting-file : taskfile : taskfile.thats.not.there.yaml optional : true Note that from-nonexisting-file include is optional, so the taskfile works fine even if the file's not there. Now you know how to include external Taskfiles. Note: You can't chain includes , like including the taskfile that already includes other taskfiles. You can walkaround this limitation calling tasks from another taskfiles with commands, like task -t taskfile-name.yaml taskname , that's where this syntax comes in handy. But whenever you can avoid complexity, please do. Next chapter","title":"Optional includes"},{"location":"base-tutorials/a-tasker/c04_vars/","text":"Variables \u00b6 Variables in official documentation Intro to Variables \u00b6 Here is the Taskfile to follow along. Open the working directory in terminal (I suggest you to use ./working_dir ) As you may already notice, tasks can make use of variables. For example, let's create an initial taskfile in our working directory: task --init . Open the Taskfile.yaml in a text editor, and you will see: # https://taskfile.dev version : '3' vars : GREETING : Hello, World! tasks : default : cmds : - echo \"{{.GREETING}}\" silent : true Global Variables \u00b6 vars section is indeed for variables, and one variable is declared there. GREETING is the name of the variable, Hello, World! is its value. This is a global variable that is declared on the level of a whole taskfile. Variables are useful in cases when a bunch of tasks use the same value. You can assign this value to a variable so when you'll need to edit it you'll have to edit it just once. Let's write a task that uses the same global variable that we already have: greet-global : desc : run this task to print the global GREETING variable declared above cmds : - echo {{.GREETING}} Call this task: task greet-global . Does it print out Hello, World! ? Fine. Local Variables \u00b6 Meanwhile each task can have its own variables. Let's create an example: greet-local : desc : run this task to print the local GREETING variable declared in task vars : GREETING : Hi! This string is the value of a local variable cmds : - echo {{.GREETING}} Run this task: task greet-local . Notice that the local variable overrides the global one with the same name. By the way: When you call a task from another task, the caller's variable overrides the callie's variable with the same name. Now let's see how taskfiles deal with environmental variables. Next chapter","title":"Variables"},{"location":"base-tutorials/a-tasker/c04_vars/#variables","text":"Variables in official documentation","title":"Variables"},{"location":"base-tutorials/a-tasker/c04_vars/#intro-to-variables","text":"Here is the Taskfile to follow along. Open the working directory in terminal (I suggest you to use ./working_dir ) As you may already notice, tasks can make use of variables. For example, let's create an initial taskfile in our working directory: task --init . Open the Taskfile.yaml in a text editor, and you will see: # https://taskfile.dev version : '3' vars : GREETING : Hello, World! tasks : default : cmds : - echo \"{{.GREETING}}\" silent : true","title":"Intro to Variables"},{"location":"base-tutorials/a-tasker/c04_vars/#global-variables","text":"vars section is indeed for variables, and one variable is declared there. GREETING is the name of the variable, Hello, World! is its value. This is a global variable that is declared on the level of a whole taskfile. Variables are useful in cases when a bunch of tasks use the same value. You can assign this value to a variable so when you'll need to edit it you'll have to edit it just once. Let's write a task that uses the same global variable that we already have: greet-global : desc : run this task to print the global GREETING variable declared above cmds : - echo {{.GREETING}} Call this task: task greet-global . Does it print out Hello, World! ? Fine.","title":"Global Variables"},{"location":"base-tutorials/a-tasker/c04_vars/#local-variables","text":"Meanwhile each task can have its own variables. Let's create an example: greet-local : desc : run this task to print the local GREETING variable declared in task vars : GREETING : Hi! This string is the value of a local variable cmds : - echo {{.GREETING}} Run this task: task greet-local . Notice that the local variable overrides the global one with the same name. By the way: When you call a task from another task, the caller's variable overrides the callie's variable with the same name. Now let's see how taskfiles deal with environmental variables. Next chapter","title":"Local Variables"},{"location":"base-tutorials/a-tasker/c05_env_vars/","text":"Environment Variables \u00b6 Environment Variables in official documentation Intro to Environment Variables \u00b6 Here is the Taskfile to follow along. Open the working directory in terminal (I suggest you to use ./working_dir ) Tasks can make use of envronmental variables. Let's explore this feature. Create a new taskfile ( task --init ), open it in a text editor and add a new task to it: greet-env : desc : here we declare an enviroment variable right here in task cmds : - echo ${GREETING} env : GREETING : Hello, dear user! Here is the environment variable Here we dectale the environment variable in env block and call it like this: ${GREETING} Try to run this task: task greet-env Global Environment Variables \u00b6 This is how we declare environment variables in tasks. We can declare them on the taskfile level as well. Let's do it. Add an env block after the vars block in the beginning of the file: version : '3' vars : GREETING : Here we explore using enviroment variables in tasks env : GLOBAL_GREET_ENV : This env var is accessible for all tasks Add a task that will use this variable: greet-global-env : desc : here we use an enviroment variable declared globally in the taskfile cmds : - echo ${GLOBAL_GREET_ENV} Try it: task greet-global-env Environment Variables Priority \u00b6 Local enviroment variables override the global ones. Add this task to the taskfile: greet-global-env-override : desc : here we override an enviroment variable declared globally above cmds : - echo ${GLOBAL_GREET_ENV} env : GLOBAL_GREET_ENV : A local env var overrides the global one Run it: task greet-global-env-override . Passing Environment Variables in Command Line \u00b6 Now let's try to pass an enviroment variable in the command line while calling the task: GREETING=\"Hi from command line\" task greet-env And with the global variable it will work as well (sure enough): GLOBAL_GREET_ENV=\"And this one is from command line\" task greet-global-env And even with both of them: GLOBAL_GREET_ENV=\"From command line as well\" task greet-global-env-override That's expected: the enviroment variable that comes from the enviroment is more important than the one defined in the taskfile. Using Existing Environment Variables in Tasks \u00b6 Let's use an enviroment variable that we definitely have. Add a task to the taskfile: shell-name : desc : In this task we use existing enviroment variable cmds : - echo ${SHELL} Run it ( task shell-name ), and it will print out the shell path. When Variables Get Evaluated to Environment Variables \u00b6 If task uses a variable with the same name that an enviroment variable has, and no variables with this name were declared in taskfile or task itself, this variable gets evaluated to the value of the enviroment variable with the same name. For example let's add a task: shell-name-as-var : desc : This task will evaluate a var to the env var from system cmds : - echo {{.SHELL}} Note that {{.SHELL}} is written like an ordinary variable, not like this: ${SHELL} . But the result in this case will be the same! Anyway to avoid confusion I recommend to use ${ENV_VAR} syntax for enviroment variables and {{.VAR}} for other variables. Look what will happen if we'll declare a variable {{.SHELL}} in this task: shell-var : desc : This task won't evaluate a var to the env var from system cmds : - echo {{.SHELL}} vars : SHELL : var overrides an env var if called as a var In this case it behaves like an ordinary variable. Name Collisions Between Variables and Environment Variables \u00b6 Let's add a task that uses a local enviroment variable and a global variable with the same name (it will work with a local variable as well): greet-env-and-var : desc : global and enviroment variable do not fight, but avoid name collisions cmds : - echo \"{{.GREETING}}\" - echo ${GREETING} env : GREETING : The environment variable doesn't override the global variable Run it ( task greet-env-and-var ), and you'll see both values printed out. They won't override each other, but nontheless it's not recommended to collide variable names like this. If you can avoid confusion, do. This was a long chapter with many sections and edge cases, because its subject is pretty complicated. The next chapter will be much easier! Next chapter","title":"Environment Variables"},{"location":"base-tutorials/a-tasker/c05_env_vars/#environment-variables","text":"Environment Variables in official documentation","title":"Environment Variables"},{"location":"base-tutorials/a-tasker/c05_env_vars/#intro-to-environment-variables","text":"Here is the Taskfile to follow along. Open the working directory in terminal (I suggest you to use ./working_dir ) Tasks can make use of envronmental variables. Let's explore this feature. Create a new taskfile ( task --init ), open it in a text editor and add a new task to it: greet-env : desc : here we declare an enviroment variable right here in task cmds : - echo ${GREETING} env : GREETING : Hello, dear user! Here is the environment variable Here we dectale the environment variable in env block and call it like this: ${GREETING} Try to run this task: task greet-env","title":"Intro to Environment Variables"},{"location":"base-tutorials/a-tasker/c05_env_vars/#global-environment-variables","text":"This is how we declare environment variables in tasks. We can declare them on the taskfile level as well. Let's do it. Add an env block after the vars block in the beginning of the file: version : '3' vars : GREETING : Here we explore using enviroment variables in tasks env : GLOBAL_GREET_ENV : This env var is accessible for all tasks Add a task that will use this variable: greet-global-env : desc : here we use an enviroment variable declared globally in the taskfile cmds : - echo ${GLOBAL_GREET_ENV} Try it: task greet-global-env","title":"Global Environment Variables"},{"location":"base-tutorials/a-tasker/c05_env_vars/#environment-variables-priority","text":"Local enviroment variables override the global ones. Add this task to the taskfile: greet-global-env-override : desc : here we override an enviroment variable declared globally above cmds : - echo ${GLOBAL_GREET_ENV} env : GLOBAL_GREET_ENV : A local env var overrides the global one Run it: task greet-global-env-override .","title":"Environment Variables Priority"},{"location":"base-tutorials/a-tasker/c05_env_vars/#passing-environment-variables-in-command-line","text":"Now let's try to pass an enviroment variable in the command line while calling the task: GREETING=\"Hi from command line\" task greet-env And with the global variable it will work as well (sure enough): GLOBAL_GREET_ENV=\"And this one is from command line\" task greet-global-env And even with both of them: GLOBAL_GREET_ENV=\"From command line as well\" task greet-global-env-override That's expected: the enviroment variable that comes from the enviroment is more important than the one defined in the taskfile.","title":"Passing Environment Variables in Command Line"},{"location":"base-tutorials/a-tasker/c05_env_vars/#using-existing-environment-variables-in-tasks","text":"Let's use an enviroment variable that we definitely have. Add a task to the taskfile: shell-name : desc : In this task we use existing enviroment variable cmds : - echo ${SHELL} Run it ( task shell-name ), and it will print out the shell path.","title":"Using Existing Environment Variables in Tasks"},{"location":"base-tutorials/a-tasker/c05_env_vars/#when-variables-get-evaluated-to-environment-variables","text":"If task uses a variable with the same name that an enviroment variable has, and no variables with this name were declared in taskfile or task itself, this variable gets evaluated to the value of the enviroment variable with the same name. For example let's add a task: shell-name-as-var : desc : This task will evaluate a var to the env var from system cmds : - echo {{.SHELL}} Note that {{.SHELL}} is written like an ordinary variable, not like this: ${SHELL} . But the result in this case will be the same! Anyway to avoid confusion I recommend to use ${ENV_VAR} syntax for enviroment variables and {{.VAR}} for other variables. Look what will happen if we'll declare a variable {{.SHELL}} in this task: shell-var : desc : This task won't evaluate a var to the env var from system cmds : - echo {{.SHELL}} vars : SHELL : var overrides an env var if called as a var In this case it behaves like an ordinary variable.","title":"When Variables Get Evaluated to Environment Variables"},{"location":"base-tutorials/a-tasker/c05_env_vars/#name-collisions-between-variables-and-environment-variables","text":"Let's add a task that uses a local enviroment variable and a global variable with the same name (it will work with a local variable as well): greet-env-and-var : desc : global and enviroment variable do not fight, but avoid name collisions cmds : - echo \"{{.GREETING}}\" - echo ${GREETING} env : GREETING : The environment variable doesn't override the global variable Run it ( task greet-env-and-var ), and you'll see both values printed out. They won't override each other, but nontheless it's not recommended to collide variable names like this. If you can avoid confusion, do. This was a long chapter with many sections and edge cases, because its subject is pretty complicated. The next chapter will be much easier! Next chapter","title":"Name Collisions Between Variables and Environment Variables"},{"location":"base-tutorials/a-tasker/c06_deps/","text":"Dependencies - Run Tasks in Parallel \u00b6 Dependencies in official documentation Intro to Dependencies - Run Tasks in Parallel \u00b6 Here is the Taskfile to follow along. Open the working directory in terminal (I suggest you to use ./working_dir ) Here's an important thing: Dependencies in Taskfile is the easy way to do tasks in parallel. It's convenient in order to speed up tasks. At the same time it adds a limitation: dependencies shouldn't depend on each other. Let's run something in parallel. Create a new taskfile: task --init . Open it with a text editor and add three new tasks there: download : desc : download datasets in parallel deps : - dataset1 - dataset2 dataset1 : desc : download dataset 1 cmds : - wget https://eforexcel.com/wp/wp-content/uploads/2017/07/5000-Sales-Records.zip dataset2 : desc : download dataset 2 cmds : - wget https://eforexcel.com/wp/wp-content/uploads/2017/07/10000-Sales-Records.zip The fisrt task calls the second and the third in parallel. Run it and see: task download . Okay, now we have our datasets. But they are packed into zip archives. Let's unpack them, also in parallel. Meanwhile we'll get familiar with an alternative syntax for lists in taskfiles. Add tasks to the taskfile: extract : desc : extract datasets in parallel deps : [ extract1 , extract2 ] extract1 : desc : extract dataset 1 cmds : - unzip 5000-Sales-Records.zip extract2 : desc : extract dataset 2 cmds : - unzip 10000-Sales-Records.zip Note this line: deps: [extract1, extract2] It is equivalent to the list of lines starting with dashes like this: deps : - extract1 - extract2 I don't recommend to write commands in tasks in this form to keep them human readable, but for list of dependencies it's fine, as it consists of short words. There's another important thing about dependencies: they run before the commands of the task that depend on them. By the way, we will return to those datasets in further chapters. Next chapter","title":"Dependencies - Run Tasks in Parallel"},{"location":"base-tutorials/a-tasker/c06_deps/#dependencies-run-tasks-in-parallel","text":"Dependencies in official documentation","title":"Dependencies - Run Tasks in Parallel"},{"location":"base-tutorials/a-tasker/c06_deps/#intro-to-dependencies-run-tasks-in-parallel","text":"Here is the Taskfile to follow along. Open the working directory in terminal (I suggest you to use ./working_dir ) Here's an important thing: Dependencies in Taskfile is the easy way to do tasks in parallel. It's convenient in order to speed up tasks. At the same time it adds a limitation: dependencies shouldn't depend on each other. Let's run something in parallel. Create a new taskfile: task --init . Open it with a text editor and add three new tasks there: download : desc : download datasets in parallel deps : - dataset1 - dataset2 dataset1 : desc : download dataset 1 cmds : - wget https://eforexcel.com/wp/wp-content/uploads/2017/07/5000-Sales-Records.zip dataset2 : desc : download dataset 2 cmds : - wget https://eforexcel.com/wp/wp-content/uploads/2017/07/10000-Sales-Records.zip The fisrt task calls the second and the third in parallel. Run it and see: task download . Okay, now we have our datasets. But they are packed into zip archives. Let's unpack them, also in parallel. Meanwhile we'll get familiar with an alternative syntax for lists in taskfiles. Add tasks to the taskfile: extract : desc : extract datasets in parallel deps : [ extract1 , extract2 ] extract1 : desc : extract dataset 1 cmds : - unzip 5000-Sales-Records.zip extract2 : desc : extract dataset 2 cmds : - unzip 10000-Sales-Records.zip Note this line: deps: [extract1, extract2] It is equivalent to the list of lines starting with dashes like this: deps : - extract1 - extract2 I don't recommend to write commands in tasks in this form to keep them human readable, but for list of dependencies it's fine, as it consists of short words. There's another important thing about dependencies: they run before the commands of the task that depend on them. By the way, we will return to those datasets in further chapters. Next chapter","title":"Intro to Dependencies - Run Tasks in Parallel"},{"location":"base-tutorials/a-tasker/c07_cli_args/","text":"Passing CLI Arguments to the Task \u00b6 CLI Arguments in official documentation Intro to Passing CLI Arguments to the Task \u00b6 Here is the Taskfile to follow along. Open the working directory in terminal (I suggest you to use ./working_dir ) Remember that you can pass an environment variable to the task in command line? You might as well use CLI arguments as a special variable inside your tasks. Let's learn how. Inside the task this varibale will look like this: {{.CLI_ARGS}} Create a new taskfile ( task --init ) and add a task to it: greet-cli : desc : 'usage: task greet-cli -- username' cmds : - echo \"Hello dear {{.CLI_ARGS}}\" Call that task with your name as an argument like this: task greet-cli -- username It should greet you by username. Obviously this feature is useful in more realistic applications as well. Like one in the next chapter . Next chapter","title":"Passing CLI Arguments to the Task"},{"location":"base-tutorials/a-tasker/c07_cli_args/#passing-cli-arguments-to-the-task","text":"CLI Arguments in official documentation","title":"Passing CLI Arguments to the Task"},{"location":"base-tutorials/a-tasker/c07_cli_args/#intro-to-passing-cli-arguments-to-the-task","text":"Here is the Taskfile to follow along. Open the working directory in terminal (I suggest you to use ./working_dir ) Remember that you can pass an environment variable to the task in command line? You might as well use CLI arguments as a special variable inside your tasks. Let's learn how. Inside the task this varibale will look like this: {{.CLI_ARGS}} Create a new taskfile ( task --init ) and add a task to it: greet-cli : desc : 'usage: task greet-cli -- username' cmds : - echo \"Hello dear {{.CLI_ARGS}}\" Call that task with your name as an argument like this: task greet-cli -- username It should greet you by username. Obviously this feature is useful in more realistic applications as well. Like one in the next chapter . Next chapter","title":"Intro to Passing CLI Arguments to the Task"},{"location":"base-tutorials/a-tasker/c08_requests/","text":"Requests - an Applied Taskfile Examples \u00b6 Here is the Taskfile to follow along. Open the working directory in terminal (I suggest you to use ./working_dir ) Remember we've fetched some example datasets from the internet in chapter 06 ? Let's do it once again. Create a taskfile: task --init . Add tasks: download-dataset : desc : download a dataset cmds : - wget https://eforexcel.com/wp/wp-content/uploads/2017/07/10000-Sales-Records.zip extract-dataset : desc : extract dataset deps : download-dataset cmds : - unzip 10000-Sales-Records.zip Note that we add a dependency to extract-dataset so when we call this task it first executes the download-dataset task. Call the extract task: task extract-dataset See what happened: first the download-dataset task is executed and only then the extract-dataset runs. Nice! Now we have an example dataset to query. This randomly-generated dataset mimics global sales data. Open the .csv file to see its content format. Say we need a task that will print out sales of particular product in particular region of the world. Let's use a taskfile variables: one to set the region, one to set the product. Add to the vars block of the taskfile: vars : GREETING : Hello, World! REGION : Asia PRODUCT : Cereal Add a task that will use these variables to query our dataset using grep : query-with-global-vars : desc : query a dataset with the variables set globally above cmds : - cat \"10000 Sales Records.csv\" | grep \"{{.REGION}}\" | grep {{.PRODUCT}} Fine. Now we can make a task that will accept an additional CLI argument to focus our request. It will use the same variables for region and product, but it will take an additional argument as well. Note the multi-line syntax: query-with-cli-args : desc : query a dataset with the variables set globally above cmds : - |- cat \"10000 Sales Records.csv\" \\ | grep \"{{.REGION}}\" | grep {{.PRODUCT}} \\ | grep {{.CLI_ARGS}} Try to call it with a country name as an additional argument, for instance: task query-with-cli-args -- Kazakhstan Without a CLI argument it will throw an error, because grep need one argument. Now let's make another task with local variables overriding global ones: query-with-local-vars : desc : query a dataset with the variables set globally above cmds : - cat \"10000 Sales Records.csv\" | grep \"{{.REGION}}\" | grep {{.PRODUCT}} vars : REGION : Europe PRODUCT : Meat Try this task task query-with-local-vars and see data on meat sales in Europe. Let's make a task that will save our query results into a file, and the name of this file will be passed as a CLI argumet: query-save-to-file : desc : query a dataset with the variables set globally above cmds : - |- cat \"10000 Sales Records.csv\" \\ | grep \"{{.REGION}}\" | grep {{.PRODUCT}} \\ > {{.CLI_ARGS}} vars : REGION : Europe PRODUCT : Meat Call this task with an output file name as a CLI argument, for instance: task query-save-to-file -- euro-meat.csv Now you can open the output file and make sure that it contains the result of our query. That's it! \u00b6 Thank you for attending this tutorial. Now you know how to use Taskfile. Feel free to apply this knowledge. Hope Taskfile will appear a useful tool for you and this tutorial helped you to learn its features and feel its human-friendly approach to task automation.","title":"Requests - an Applied Taskfile Examples"},{"location":"base-tutorials/a-tasker/c08_requests/#requests-an-applied-taskfile-examples","text":"Here is the Taskfile to follow along. Open the working directory in terminal (I suggest you to use ./working_dir ) Remember we've fetched some example datasets from the internet in chapter 06 ? Let's do it once again. Create a taskfile: task --init . Add tasks: download-dataset : desc : download a dataset cmds : - wget https://eforexcel.com/wp/wp-content/uploads/2017/07/10000-Sales-Records.zip extract-dataset : desc : extract dataset deps : download-dataset cmds : - unzip 10000-Sales-Records.zip Note that we add a dependency to extract-dataset so when we call this task it first executes the download-dataset task. Call the extract task: task extract-dataset See what happened: first the download-dataset task is executed and only then the extract-dataset runs. Nice! Now we have an example dataset to query. This randomly-generated dataset mimics global sales data. Open the .csv file to see its content format. Say we need a task that will print out sales of particular product in particular region of the world. Let's use a taskfile variables: one to set the region, one to set the product. Add to the vars block of the taskfile: vars : GREETING : Hello, World! REGION : Asia PRODUCT : Cereal Add a task that will use these variables to query our dataset using grep : query-with-global-vars : desc : query a dataset with the variables set globally above cmds : - cat \"10000 Sales Records.csv\" | grep \"{{.REGION}}\" | grep {{.PRODUCT}} Fine. Now we can make a task that will accept an additional CLI argument to focus our request. It will use the same variables for region and product, but it will take an additional argument as well. Note the multi-line syntax: query-with-cli-args : desc : query a dataset with the variables set globally above cmds : - |- cat \"10000 Sales Records.csv\" \\ | grep \"{{.REGION}}\" | grep {{.PRODUCT}} \\ | grep {{.CLI_ARGS}} Try to call it with a country name as an additional argument, for instance: task query-with-cli-args -- Kazakhstan Without a CLI argument it will throw an error, because grep need one argument. Now let's make another task with local variables overriding global ones: query-with-local-vars : desc : query a dataset with the variables set globally above cmds : - cat \"10000 Sales Records.csv\" | grep \"{{.REGION}}\" | grep {{.PRODUCT}} vars : REGION : Europe PRODUCT : Meat Try this task task query-with-local-vars and see data on meat sales in Europe. Let's make a task that will save our query results into a file, and the name of this file will be passed as a CLI argumet: query-save-to-file : desc : query a dataset with the variables set globally above cmds : - |- cat \"10000 Sales Records.csv\" \\ | grep \"{{.REGION}}\" | grep {{.PRODUCT}} \\ > {{.CLI_ARGS}} vars : REGION : Europe PRODUCT : Meat Call this task with an output file name as a CLI argument, for instance: task query-save-to-file -- euro-meat.csv Now you can open the output file and make sure that it contains the result of our query.","title":"Requests - an Applied Taskfile Examples"},{"location":"base-tutorials/a-tasker/c08_requests/#thats-it","text":"Thank you for attending this tutorial. Now you know how to use Taskfile. Feel free to apply this knowledge. Hope Taskfile will appear a useful tool for you and this tutorial helped you to learn its features and feel its human-friendly approach to task automation.","title":"That's it!"},{"location":"base-tutorials/b-devpod/","text":"About \u00b6 Devpod makes it easy to prepare an environment for software development. It works the same way on cloud computing instances and docker containers. Project Development View source on Github | Develop on Gitpod | Read docs at Get Started with Devpod AWS EC2 Docker Usage: Get Started with Devpod","title":"About"},{"location":"base-tutorials/b-devpod/#about","text":"Devpod makes it easy to prepare an environment for software development. It works the same way on cloud computing instances and docker containers. Project Development View source on Github | Develop on Gitpod | Read docs at Get Started with Devpod AWS EC2 Docker Usage: Get Started with Devpod","title":"About"},{"location":"base-tutorials/b-devpod/devpod-get-env-ready/","text":"Get Started \u00b6 Devpod makes it easy to prepare an environment for software development. It works the same way on cloud computing instances like AWS EC2, in Docker containers or on desktop machines. Note that Devpod requires internet access in order to work. Quick Install \u00b6 There are two ways to Quick Install the Devpod: Quick Setup Base and Quick Toolset Setup Quick Setup Base \u00b6 To install the base toolset first and the rest later, do this: echo \"download and run the $pwd init script\" wget -q -O - https://raw.githubusercontent.com/yairdar/devpod/main/init.sh | bash echo \"run idempotent setup process, only missing tools will be installed\" bash devpod/install-deps.sh setup-base Following the Quick Install procedure you can install a minimal Developer Toolset. It contains some basic utilities ( curl wget git vim zip unzip ), task (aka Taskfile ), yq and zsh with oh my zsh extension framework and task autocompletions. Taskfile is the main automation tool in Devpod. It allows to run complex multistep tasks with convenient commands in task taskname form. Taskfile uses YAML to describe the tasks, and it makes automation easy and intuitive. That's why Devpod itself relies on Taskfile. Yq is a YAML processor that supports JSON as well. with yq we can use task to introspect other tasks or yaml/json configurations that gives it a lot of power Zsh is a powerful shell that supports plugins, has an advanced autocompletion, shorthands for most commonly used commands and vast customization capabilities. Oh my zsh framework helps managing zsh configuration and and adds even more convenience. Task autocompletions for zsh allow to autocomplete task names after task command. In addition to this basic set you can install Custom Devops Parts that you need with following commands: Cloud Tools (GitHub CLI, AWS CLI) task setup-cloud-tools Conda task setup-os-conda Docker task setup-os-docker echo \"install custom devpod parts\" cd devpod task -p -o prefixed setup-cloud-tools setup-os-conda setup-os-docker # drop -p to disable parallelism Quick Toolset Setup \u00b6 If you want to install any of Custom Parts right from the start, you can add arguments to the Quick Install Command like this: echo \"download and run the $pwd init script\" wget -q -O - https://raw.githubusercontent.com/yairdar/devpod/main/init.sh | bash echo \"run idempotent setup process, only missing tools will be installed\" bash devpod/install-deps.sh setup-base setup-cloud-tools setup-os-docker Arguments to choose from: Cloud Tools (GitHub CLI, AWS CLI) setup-cloud-tools Conda setup-os-conda Docker setup-os-docker How to install Devpod to the Docker container \u00b6 Including Devpod to the Dockerfile is pretty straightforward: RUN bash devpod/install-deps.sh setup-base <SETUP_CUSTOM_PARTS> where <SETUP_CUSTOM_PARTS> can include setup-cloud-tools , setup-os-conda and setup-os-docker . You can see the example here . (update this line after merge) The only consideration could be if you need task comlpetion for zsh in your Docker container or not. If you do and your container doesn't include python3 you should setup it manually: open your ~/.zshrc file and add task to plugins list: plugins=(git task) autoload -U compinit && compinit source $ZSH/oh-my-zsh.sh In most cases this feature is not that useful in Docker containers. How to install Devpod to the cloud node \u00b6 The procedure is pretty much the same for the cloud node that's for a local machine or a remote server. Connect to the node over SSH and use any of two Quick Install procedures. How to tune Devpod for your project \u00b6 You can add a custom taskfile to the devpod. It allows to minimize manual work while preparing your systems for development. The install-nodejs task example in this chapeter is realistic and not simplified. If you want to learn how to use taskfiles, here's a comprehensive tutorial . Here's how to add a custom installation automation to Devpod: Create a taskfile in devpod directory with your favorite text editor, e.g.: vim setup.custom.tools.yaml Copy to this file: # https://taskfile.dev version : \"3\" vars : MAYBE_SUDO : sh : which sudo &> /dev/null && echo \"sudo\" || echo \"\" tasks : default : cmds : - echo \"{{.GREETING}}\" - task -a silent : true Note MAYBE_SUDO monade. It allows to run the same script on systems with and without sudo. Create your custom tools installation task, for example: tasks : default : cmds : - echo \"{{.GREETING}}\" - task -a silent : true install-nodejs : desc : install node.js vars : DISTRO : linux-x64 VERSION : v16.17.0 cmds : - curl https://nodejs.org/dist/{{.VERSION}}/node-{{.VERSION}}-{{.DISTRO}}.tar.xz -o node.tar.xz - {{ .MAYBE_SUDO }} mkdir -p /usr/local/lib/nodejs - {{ .MAYBE_SUDO }} tar -xJvf node.tar.xz -C /usr/local/lib/nodejs - echo \"export PATH=/usr/local/lib/nodejs/node-{{.VERSION}}-{{.DISTRO}}/bin:\\$PATH\" > ~/.profile - ~/.profile Add a task to the Taskfile.yml that will call your new taskfile: setup-custom-tools : desc : Install node.js cmds : - task -t setup.custom.tools.yaml install-nodejs Now you run Devpod installation like this to install nodejs with it: bash devpod/install-deps.sh setup-base install-nodejs How to contribute \u00b6 You are welcome to offer contributions to Devpod via pull requests to the GitHub repo . If you have written a custom taskfile with automation for installation of a common development tool, it's definitely worth sharing with community.","title":"Get Started"},{"location":"base-tutorials/b-devpod/devpod-get-env-ready/#get-started","text":"Devpod makes it easy to prepare an environment for software development. It works the same way on cloud computing instances like AWS EC2, in Docker containers or on desktop machines. Note that Devpod requires internet access in order to work.","title":"Get Started"},{"location":"base-tutorials/b-devpod/devpod-get-env-ready/#quick-install","text":"There are two ways to Quick Install the Devpod: Quick Setup Base and Quick Toolset Setup","title":"Quick Install"},{"location":"base-tutorials/b-devpod/devpod-get-env-ready/#quick-setup-base","text":"To install the base toolset first and the rest later, do this: echo \"download and run the $pwd init script\" wget -q -O - https://raw.githubusercontent.com/yairdar/devpod/main/init.sh | bash echo \"run idempotent setup process, only missing tools will be installed\" bash devpod/install-deps.sh setup-base Following the Quick Install procedure you can install a minimal Developer Toolset. It contains some basic utilities ( curl wget git vim zip unzip ), task (aka Taskfile ), yq and zsh with oh my zsh extension framework and task autocompletions. Taskfile is the main automation tool in Devpod. It allows to run complex multistep tasks with convenient commands in task taskname form. Taskfile uses YAML to describe the tasks, and it makes automation easy and intuitive. That's why Devpod itself relies on Taskfile. Yq is a YAML processor that supports JSON as well. with yq we can use task to introspect other tasks or yaml/json configurations that gives it a lot of power Zsh is a powerful shell that supports plugins, has an advanced autocompletion, shorthands for most commonly used commands and vast customization capabilities. Oh my zsh framework helps managing zsh configuration and and adds even more convenience. Task autocompletions for zsh allow to autocomplete task names after task command. In addition to this basic set you can install Custom Devops Parts that you need with following commands: Cloud Tools (GitHub CLI, AWS CLI) task setup-cloud-tools Conda task setup-os-conda Docker task setup-os-docker echo \"install custom devpod parts\" cd devpod task -p -o prefixed setup-cloud-tools setup-os-conda setup-os-docker # drop -p to disable parallelism","title":"Quick Setup Base"},{"location":"base-tutorials/b-devpod/devpod-get-env-ready/#quick-toolset-setup","text":"If you want to install any of Custom Parts right from the start, you can add arguments to the Quick Install Command like this: echo \"download and run the $pwd init script\" wget -q -O - https://raw.githubusercontent.com/yairdar/devpod/main/init.sh | bash echo \"run idempotent setup process, only missing tools will be installed\" bash devpod/install-deps.sh setup-base setup-cloud-tools setup-os-docker Arguments to choose from: Cloud Tools (GitHub CLI, AWS CLI) setup-cloud-tools Conda setup-os-conda Docker setup-os-docker","title":"Quick Toolset Setup"},{"location":"base-tutorials/b-devpod/devpod-get-env-ready/#how-to-install-devpod-to-the-docker-container","text":"Including Devpod to the Dockerfile is pretty straightforward: RUN bash devpod/install-deps.sh setup-base <SETUP_CUSTOM_PARTS> where <SETUP_CUSTOM_PARTS> can include setup-cloud-tools , setup-os-conda and setup-os-docker . You can see the example here . (update this line after merge) The only consideration could be if you need task comlpetion for zsh in your Docker container or not. If you do and your container doesn't include python3 you should setup it manually: open your ~/.zshrc file and add task to plugins list: plugins=(git task) autoload -U compinit && compinit source $ZSH/oh-my-zsh.sh In most cases this feature is not that useful in Docker containers.","title":"How to install Devpod to the Docker container"},{"location":"base-tutorials/b-devpod/devpod-get-env-ready/#how-to-install-devpod-to-the-cloud-node","text":"The procedure is pretty much the same for the cloud node that's for a local machine or a remote server. Connect to the node over SSH and use any of two Quick Install procedures.","title":"How to install Devpod to the cloud node"},{"location":"base-tutorials/b-devpod/devpod-get-env-ready/#how-to-tune-devpod-for-your-project","text":"You can add a custom taskfile to the devpod. It allows to minimize manual work while preparing your systems for development. The install-nodejs task example in this chapeter is realistic and not simplified. If you want to learn how to use taskfiles, here's a comprehensive tutorial . Here's how to add a custom installation automation to Devpod: Create a taskfile in devpod directory with your favorite text editor, e.g.: vim setup.custom.tools.yaml Copy to this file: # https://taskfile.dev version : \"3\" vars : MAYBE_SUDO : sh : which sudo &> /dev/null && echo \"sudo\" || echo \"\" tasks : default : cmds : - echo \"{{.GREETING}}\" - task -a silent : true Note MAYBE_SUDO monade. It allows to run the same script on systems with and without sudo. Create your custom tools installation task, for example: tasks : default : cmds : - echo \"{{.GREETING}}\" - task -a silent : true install-nodejs : desc : install node.js vars : DISTRO : linux-x64 VERSION : v16.17.0 cmds : - curl https://nodejs.org/dist/{{.VERSION}}/node-{{.VERSION}}-{{.DISTRO}}.tar.xz -o node.tar.xz - {{ .MAYBE_SUDO }} mkdir -p /usr/local/lib/nodejs - {{ .MAYBE_SUDO }} tar -xJvf node.tar.xz -C /usr/local/lib/nodejs - echo \"export PATH=/usr/local/lib/nodejs/node-{{.VERSION}}-{{.DISTRO}}/bin:\\$PATH\" > ~/.profile - ~/.profile Add a task to the Taskfile.yml that will call your new taskfile: setup-custom-tools : desc : Install node.js cmds : - task -t setup.custom.tools.yaml install-nodejs Now you run Devpod installation like this to install nodejs with it: bash devpod/install-deps.sh setup-base install-nodejs","title":"How to tune Devpod for your project"},{"location":"base-tutorials/b-devpod/devpod-get-env-ready/#how-to-contribute","text":"You are welcome to offer contributions to Devpod via pull requests to the GitHub repo . If you have written a custom taskfile with automation for installation of a common development tool, it's definitely worth sharing with community.","title":"How to contribute"},{"location":"contribution/","text":"Contribution \u00b6 A guide to Sowftware Gardening Get Started \u00b6 We consider that you have conda or python installed and working environment activated. # clone this repo git clone https://github.com/yairdar/aguide.git # or via ssh with arg=git@github.com:yairdar/aguide.git # install mkdocs deps in current envoronment task docs-builder:install-pip-deps # try to build site from source task docs-builder:build-site Now we can see documentation using task docs-builder:serve-source Autp Port Forwarding Click on http link in vscode (with ctrl(win), alt(macos)) and it will create port forward and open link in browser Documentation Examples \u00b6 Take a look on documentation trics Nature of Software Architecture \u00b6 Guide To Comfortable Software Development Process Areas \u00b6 Execution, Compute Query Store Storage/Memory Transfer Network Publish Docs \u00b6 https://towardsdatascience.com/how-to-publish-a-python-package-to-pypi-using-poetry-aa804533fc6f https://python-poetry.org/docs/cli/ https://skerritt.blog/packaging-your-python-project/ https://gist.github.com/gkhays/a80642ecd8fe476d7220b7940fec5ad0 https://wiki.dendron.so/notes/c6fd6bc4-7f75-4cbb-8f34-f7b99bfe2d50/#vaults","title":"Contribution"},{"location":"contribution/#contribution","text":"A guide to Sowftware Gardening","title":"Contribution"},{"location":"contribution/#get-started","text":"We consider that you have conda or python installed and working environment activated. # clone this repo git clone https://github.com/yairdar/aguide.git # or via ssh with arg=git@github.com:yairdar/aguide.git # install mkdocs deps in current envoronment task docs-builder:install-pip-deps # try to build site from source task docs-builder:build-site Now we can see documentation using task docs-builder:serve-source Autp Port Forwarding Click on http link in vscode (with ctrl(win), alt(macos)) and it will create port forward and open link in browser","title":"Get Started"},{"location":"contribution/#documentation-examples","text":"Take a look on documentation trics","title":"Documentation Examples"},{"location":"contribution/#nature-of-software-architecture","text":"Guide To Comfortable Software Development Process","title":"Nature of Software Architecture"},{"location":"contribution/#areas","text":"Execution, Compute Query Store Storage/Memory Transfer Network","title":"Areas"},{"location":"contribution/#publish-docs","text":"https://towardsdatascience.com/how-to-publish-a-python-package-to-pypi-using-poetry-aa804533fc6f https://python-poetry.org/docs/cli/ https://skerritt.blog/packaging-your-python-project/ https://gist.github.com/gkhays/a80642ecd8fe476d7220b7940fec5ad0 https://wiki.dendron.so/notes/c6fd6bc4-7f75-4cbb-8f34-f7b99bfe2d50/#vaults","title":"Publish Docs"},{"location":"guides/dev-research/cv-research/","text":"CV Research \u00b6 Self Supervised Learing \u00b6 facebook's fairseq Transformers \u00b6 https://github.com/google-research/vision_transformer","title":"CV Research"},{"location":"guides/dev-research/cv-research/#cv-research","text":"","title":"CV Research"},{"location":"guides/dev-research/cv-research/#self-supervised-learing","text":"facebook's fairseq","title":"Self Supervised Learing"},{"location":"guides/dev-research/cv-research/#transformers","text":"https://github.com/google-research/vision_transformer","title":"Transformers"},{"location":"guides/dev-research/hpo-resources/","text":"HPO \u00b6 Resources \u00b6 pycaret HPO mercury","title":"HPO"},{"location":"guides/dev-research/hpo-resources/#hpo","text":"","title":"HPO"},{"location":"guides/dev-research/hpo-resources/#resources","text":"pycaret HPO mercury","title":"Resources"},{"location":"guides/dev-research/nlp-research/","text":"NLP Articles \u00b6 Full Samples \u00b6 NLP checklist marcotcr/cgecklist searching-for-semantic-similarity Summarization \u00b6 dugshub dvc simple-text-summarizer-using-extractive-method small-text sumamrization Robust Tools \u00b6 Miscrosoft unlim with publication graph Rubrix is a production-ready framework for building and improving datasets for NLP projects.","title":"NLP Articles"},{"location":"guides/dev-research/nlp-research/#nlp-articles","text":"","title":"NLP Articles"},{"location":"guides/dev-research/nlp-research/#full-samples","text":"NLP checklist marcotcr/cgecklist searching-for-semantic-similarity","title":"Full Samples"},{"location":"guides/dev-research/nlp-research/#summarization","text":"dugshub dvc simple-text-summarizer-using-extractive-method small-text sumamrization","title":"Summarization"},{"location":"guides/dev-research/nlp-research/#robust-tools","text":"Miscrosoft unlim with publication graph Rubrix is a production-ready framework for building and improving datasets for NLP projects.","title":"Robust Tools"},{"location":"guides/dev-research/timeseries-research/","text":"","title":"Timeseries research"},{"location":"guides/dev-services/github-guide/","text":"","title":"Github guide"},{"location":"guides/dev-services/gitpod-guide/","text":"gitpod \u00b6 Links \u00b6 https://hub.docker.com/r/gitpod/openvscode-server","title":"gitpod"},{"location":"guides/dev-services/gitpod-guide/#gitpod","text":"","title":"gitpod"},{"location":"guides/dev-services/gitpod-guide/#links","text":"https://hub.docker.com/r/gitpod/openvscode-server","title":"Links"},{"location":"guides/dev-tools/dev-in-container/","text":"Developing in container \u00b6 We will overview developing in container without mappings. Spin Up your container \u00b6 Just your dev image no mappings needed Setup Contaner from originator \u00b6 Now we need to sync our current credentials into dev container . # navigate into raw task-libpack cd _infra/dev-in-container-pack # Run task setup-dev-pod-from-here task setup-dev-pod-from-here #","title":"Developing in container"},{"location":"guides/dev-tools/dev-in-container/#developing-in-container","text":"We will overview developing in container without mappings.","title":"Developing in container"},{"location":"guides/dev-tools/dev-in-container/#spin-up-your-container","text":"Just your dev image no mappings needed","title":"Spin Up your container"},{"location":"guides/dev-tools/dev-in-container/#setup-contaner-from-originator","text":"Now we need to sync our current credentials into dev container . # navigate into raw task-libpack cd _infra/dev-in-container-pack # Run task setup-dev-pod-from-here task setup-dev-pod-from-here #","title":"Setup Contaner from originator"},{"location":"guides/dev-tools/dev-in-notebook/","text":"Development in notebooks \u00b6 Visualization tools \u00b6 Animated charting tool for Jupyter ipyvizzu Deploy Pycaret and Streamlit Data Science Tutorial \u00b6 Intro to Pandas 10 minutes to pandas Write SQL in notebook FugueSQL \u2014 SQL for Pandas, Spark","title":"Development in notebooks"},{"location":"guides/dev-tools/dev-in-notebook/#development-in-notebooks","text":"","title":"Development in notebooks"},{"location":"guides/dev-tools/dev-in-notebook/#visualization-tools","text":"Animated charting tool for Jupyter ipyvizzu Deploy Pycaret and Streamlit","title":"Visualization tools"},{"location":"guides/dev-tools/dev-in-notebook/#data-science-tutorial","text":"Intro to Pandas 10 minutes to pandas Write SQL in notebook FugueSQL \u2014 SQL for Pandas, Spark","title":"Data Science Tutorial"},{"location":"guides/dev-tools/vscode-extentions/","text":"VSCode Extentions Guide \u00b6 Devloper Guide \u00b6 inspired by How to Build a VS Code extension for Markdown preview using Remark processor Author : Subrat Thakur (subrat.thakur@salesforce.com) LinkedIn : https://www.linkedin.com/in/subrat-thakur/ Public Extentions \u00b6 Markdown \u00b6 Markdown Tab Complete Vs Code ```javascript I'm A tab console.log('Code Tab A'); ```javascript I'm tab B console.log('Code Tab B'); https://www.appveyor.com/docs/appveyor-yml/","title":"VSCode Extentions Guide"},{"location":"guides/dev-tools/vscode-extentions/#vscode-extentions-guide","text":"","title":"VSCode Extentions Guide"},{"location":"guides/dev-tools/vscode-extentions/#devloper-guide","text":"inspired by How to Build a VS Code extension for Markdown preview using Remark processor Author : Subrat Thakur (subrat.thakur@salesforce.com) LinkedIn : https://www.linkedin.com/in/subrat-thakur/","title":"Devloper Guide"},{"location":"guides/dev-tools/vscode-extentions/#public-extentions","text":"","title":"Public Extentions"},{"location":"guides/dev-tools/vscode-extentions/#markdown","text":"Markdown Tab Complete Vs Code ```javascript I'm A tab console.log('Code Tab A'); ```javascript I'm tab B console.log('Code Tab B'); https://www.appveyor.com/docs/appveyor-yml/","title":"Markdown"},{"location":"guides/lang-python/python-blogs/","text":"Python Blogs and REsources \u00b6 https://tryexceptpass.org/","title":"Python Blogs and REsources"},{"location":"guides/lang-python/python-blogs/#python-blogs-and-resources","text":"https://tryexceptpass.org/","title":"Python Blogs and REsources"},{"location":"guides/lang-python/python-compile/","text":"Python Compilation / Packing \u00b6 We apprcieate simple drop-in no-deps executables, which are genereated by go compiler. But some of us still don't know go, and more important there are a lot of software written in python. So we are looking for ways to pack/compile python program to single drop-in exe (or at least exe ad resource dir) Compilers Overview \u00b6 Nuitka PyInstaller mypyc cython Nuitka \u00b6 Nuitka Compile to exe on win https://github.com/Nuitka/NUITKA-Utilities/issues/62 https://github.com/Nuitka/NUITKA-Utilities/tree/master/hinted-compilation Cython \u00b6 4 Attempts at Packaging Python as an Executable PyInstaller \u00b6 https://github.com/pyinstaller/pyinstaller/blob/develop/PyInstaller/hooks/hook-pandas.py","title":"Python Compilation / Packing"},{"location":"guides/lang-python/python-compile/#python-compilation-packing","text":"We apprcieate simple drop-in no-deps executables, which are genereated by go compiler. But some of us still don't know go, and more important there are a lot of software written in python. So we are looking for ways to pack/compile python program to single drop-in exe (or at least exe ad resource dir)","title":"Python Compilation / Packing"},{"location":"guides/lang-python/python-compile/#compilers-overview","text":"Nuitka PyInstaller mypyc cython","title":"Compilers Overview"},{"location":"guides/lang-python/python-compile/#nuitka","text":"Nuitka Compile to exe on win https://github.com/Nuitka/NUITKA-Utilities/issues/62 https://github.com/Nuitka/NUITKA-Utilities/tree/master/hinted-compilation","title":"Nuitka"},{"location":"guides/lang-python/python-compile/#cython","text":"4 Attempts at Packaging Python as an Executable","title":"Cython"},{"location":"guides/lang-python/python-compile/#pyinstaller","text":"https://github.com/pyinstaller/pyinstaller/blob/develop/PyInstaller/hooks/hook-pandas.py","title":"PyInstaller"},{"location":"guides/ops-cloud/aws-guide/","text":"Aws Guide \u00b6 Better to start with https://github.com/aws-samples SSO Adventure \u00b6 https://github.com/aws-samples/aws-sso-extensions-for-enterprise 55 Weeks AWS \u00b6 https://www.linkedin.com/video/event/urn:li:ugcPost:6892101589042049025/ https://www.oreilly.com/videos/52-weeks-of/08042022VIDEOPAIML/","title":"Aws Guide"},{"location":"guides/ops-cloud/aws-guide/#aws-guide","text":"Better to start with https://github.com/aws-samples","title":"Aws Guide"},{"location":"guides/ops-cloud/aws-guide/#sso-adventure","text":"https://github.com/aws-samples/aws-sso-extensions-for-enterprise","title":"SSO Adventure"},{"location":"guides/ops-cloud/aws-guide/#55-weeks-aws","text":"https://www.linkedin.com/video/event/urn:li:ugcPost:6892101589042049025/ https://www.oreilly.com/videos/52-weeks-of/08042022VIDEOPAIML/","title":"55 Weeks AWS"},{"location":"guides/ops-cloud/k3s-guide/","text":"","title":"K3s guide"},{"location":"guides/ops-services/devops-field/","text":"Devops Field Resources \u00b6 https://devopscube.com/ Targets to taskification \u00b6 Valohai still uses Makefiles","title":"Devops Field Resources"},{"location":"guides/ops-services/devops-field/#devops-field-resources","text":"https://devopscube.com/","title":"Devops Field Resources"},{"location":"guides/ops-services/devops-field/#targets-to-taskification","text":"Valohai still uses Makefiles","title":"Targets to taskification"},{"location":"guides/ops-services/keycloak-sample/","text":"Key Cloack Examples \u00b6 Setup a production grade keycloak instance on docker","title":"Key Cloack Examples"},{"location":"guides/ops-services/keycloak-sample/#key-cloack-examples","text":"Setup a production grade keycloak instance on docker","title":"Key Cloack Examples"},{"location":"guides/ops-services/local-factory/","text":"k8s k3s minio argocd argowf \u00b6 https://github.com/sleighzy/k3s-minio-deployment","title":"k8s k3s minio argocd argowf"},{"location":"guides/ops-services/local-factory/#k8s-k3s-minio-argocd-argowf","text":"https://github.com/sleighzy/k3s-minio-deployment","title":"k8s k3s minio argocd argowf"},{"location":"guides/ops-services/minio-guilde/","text":"Minio Guide \u00b6 Rclone vs Minio \u00b6 https://www.libhunt.com/compare-minio-vs-rclone?ref=compare https://forum.rclone.org/t/recommendations-for-using-rclone-with-a-minio-10m-files/14472 https://golangrepo.com/tag/terraform-provider-minio_star_1 https://www.huuphan.com/2021/08/rclone-backup-all-vps-to-microsoft.html https://awstools.dev/ https://opensourcelibs.com/libs/s3 https://strobelstefan.org/2021/03/02/rclone-dateien-von-git-repository-automatisch-zu-nexcloud-uebertragen/ Awesome Minio \u00b6 from https://github.com/minio/awesome-minio A curated list of Minio community projects inspired by awesome-go. content A curated list of Minio community projects inspired by awesome-go . Contribution Guidelines \u00b6 Add entries alphabetically, under the appropriate category. To add, remove, or change things on the list: Submit a pull request. Description should contain a link with the name of the package/project/website. Do not exceed more than a paragraph. Cloud - PaaS \u00b6 Cloudron - Cloudron is a platform that makes it easy to run web apps on your server and keep them up-to-date. Cloudron supports storing it's backups on Minio, you can read more here . Minio itself has been packaged as a Cloudron App - you can try it in the demo (username: cloudron password: cloudron) Minio on Cloudron App Store - Package Source . Hephy Workflow (formerly known as Deis Workflow) - an open source Platform-as-a-Service for Kubernetes . Workflow uses Minio by default to store all internal application metadata and database backups. Jelastic - Jelastic is a multi-cloud PaaS and CaaS for business. It uses Minio as an S3 compatible object storage server in Docker containers. minio-dist-boshrelease - Cloud Foundry BOSH is an open source tool chain for release engineering, deployment and lifecycle management of large scale distributed services. minio-dist-boshrelease will help easily install the distributed version of Minio . minio-dokku - Dockerfile to run Minio on Dokku PaaS. Rafter - Kubernetes-native files/assets store powered by MinIO. It extends Kubernetes with Custom Resource Definitions like AssetGroup, Asset and Bucket. It supports webhook approach that you can modify/validate/analyze a file asynchronously before it is being saved into MinIO. sloppy.io - sloppy.io is the fastest way to deploy your docker container online. We provide the infrastructure and workflow to run container applications and micro services. Access our platform over the web, via CLI or our own API. Check out Deploying Minio to sloppy.io for further info! Univention Corporate Server - Univention Corporate Server (UCS) is the innovative basis for the cost-efficient operation and easy administration of server applications and entire IT infrastructures. The integrated App Center offers a multitude of enterprise solutions, which can be run or operated virtually with just a few clicks. Minio has been packages as an app for Univention - you can try it from the Univention App Center - Source . Cloud - IaaS \u00b6 Digital Ocean - Deploy an SSD cloud server in 55 seconds. Eucalyptus - Eucalyptus is an open solution to build private and hybrid clouds that are compatible with Amazon Web Services. Minio can be used as an object storage backend for Eucalyptus. Ha-Minio - Configures Minio, as a S3 compatible cloud storage server for DigitalOcean in a highly available fashion. Onlinetech - Secure, compliant enterprise cloud. OVH - Build your own infrastructure with OVH public cloud.. Packet - Packet is a baremetal cloud provider. Plumbery - Plumbery is an open source project that was initiated by Dimension Data to accelerate digital transformation. It recommends using Minio as a standalone object storage. s3-tus-store - Minio is the supported storage backend for https://tus.io/ . Tus is an open protocol for resumable file uploads. ScaleWay - Scalable and affordable cloud and bare-metal servers (European DC). SSD Nodes - Simple, high performance cloud provider with truly personalized support. DevOps - CI/CD \u00b6 drone.io - drone.io is continuous integration for Github and Bitbucket, that monitors your code for bugs. It uses Minio as an AWS S3 cloud storage alternative. Gorbachev IO - Gorbachev is a continuous integration platform, used to manage reproducible research at Dragonfly Data Science. Minio tools are awesome, and the community is supportive and responsive. mkrepo - maintains an RPM or DEB repository in S3, and periodically regenerates metadata. You can use Minio as a convenient and inexpensive storage for your CI. s3-resource-simple - s3-resource-simple is Concourse CI's resource for uploading files to S3. It supports Minio as an object storage backend. puppet-minio - Puppet module to manage Minio installations. Uses the binary, does not (yet) support Docker based installation. (Requires Puppet version 4+) Cloud/SaaS Applications \u00b6 Appknox - Appknox detects & addresses vulnerabilities in mobile apps within minutes. Managing storage was a nightmare for us before Minio came along. We use Minio to store all our ipa/apk files and pdf reports that gets auto-nenerated. We are an enterprise startup and we offer on-premise and private-cloud installations of our cloud-based scanner. We have deployed our code base on AWS, Google, Azure, multiple on-premise installations and private cloud. Before Mino came along - we had to write and maintain Python libraries for each platform and maintain them. Some of the changes that we make cannot be even tested - because it is deployed on-premise which we dont have access to. Thankfully, we came across Minio. Now we just install minio everywhere and forget about storage. Minio Rocks! Blockai - Blockai helps artists claim their copyrights and protects them, for free. It uses Minio for all internal development and testing as an AWS S3 compatible object storage service. Cosmic App - Cosmic App uses Minio for storing client files and quote information for commercial brokers. Files are packaged up for lenders including high street banks to access securely. Crisp - Crisp is a Customer Intelligence Platform. It helps teams know, understand and communicate with their customers, from multiple channels (chat, email). Minio is used to securely store user file uploads. Files are uploaded from the chatbox, operator dashboard and email attachments. Minio completely replaces S3 and brings more control to user data safety. dapploy - dapploy is a solution to create private app stores to deploy iOS and Android applications to your team, company or organization. Minio is easily integrated with dapploy's architecture and docker environment. Minio is used to store application packages - ipas, apks, icons and images. Minio's great feature among others is the ability to have presigned urls while downloading resources. We use that a lot. Thanks for your good documentation and community. DBHub.io - Online storage, visualisation, and collaboration for SQLite databases. Minio is the primary object store for the SQLite databases. GitLab - GitLab is an open source software to collaborate on code. It uses Minio to store objects in their caching server. Mattermost - Mattermost is an open source, self-hosted Slack alternative. It uses minio-go fully to handle S3 API requests. Pathio - Pathio is a version control system for Excel. It uses Minio as an on-premise storage backend for workbooks and json blobs. QuezX - QuezX is a recruitment aggregator for connecting employers to recruitment agencies. It uses Minio to store and manage CV's and other documents on their platform. Trustvox - Trustvox is an integrator of ecommerce platforms, ensures 100% valid product reviews, allowing only real buyers to post reviews along with a continuous audit process. The platform team chose Minio to create its own S3 service to avoid infrastructure vendor lock-in, obtaining transparent compatibility with Google Cloud Storage and previous Amazon S3 infrastructures. We tested other options before, and only Minio showed the stable and enterprise-grade results we were looking for. Rocket.Chat - Rocket.Chat is a completely FOSS alternative to Slack. In Rocket.Chat Minio can be used for file uploads \u2014 this applies to channels, groups and privately between one user and another. Content Management System \u00b6 Simple Simple Ads - uses Minio as a compatible object storage server replacement in our development environment. Multiple developers share a local repository of files transparently without having to change any site configuration. This makes developing with Drupal and the s3fs modules very easy. File changes are staged locally and then migrated to Amazon S3 after approval using the mc client utility. Application Development Services \u00b6 C0D1UM - C0D1UM is a software development firm doing projects on demand and offering operating system infrastructure support. We use minio to secure all confidential data in private network. It was a first choice because major functions are compatible with AWS S3 standard functions. Collaborne - Collaborne uses Minio docker containers in development environments to simulate a full AWS S3 environment. nxsol - nxsol specializes in the development of desktop and web-based businesses and personal applications. It uses minio-java client library for Amazon S3 related applications. ToolsLib - ToolsLib provides project management services including software hosting for millions of users. Minio fits perfectly in the current infrastructure: its ability to easily scale is a crucial point for ToolsLib services. Cloud Backup / Versioning \u00b6 Arq - Arq is a storage backend agnostic backup tool for Mac and Windows. Backend services include Amazon Cloud Drive, Google Drive, Dropbox, One Drive, Amazon S3 and more. It also supports 'Other S3-Compatible Services' which means that you can use Minio to build your own backend. BackupHive - Providing online backup services from The Netherlands with Minio as an S3 compatible back-end server to store and retrieve files. Minio is very scalable, uses almost no resources itself and is easy to maintain. The awesome team has a strong combined knowledge of use-cases, ranging from the smallest personal project to large scale cross-datacenter setups, all available within a comfortable community. burry - Burry, the BackUp & RecoveRY tool for cloud native infrastructure services enables to backup and restore ZooKeeper, etcd and Consul to and from local storage, Amazon S3, Azure Storage, Google Storage, Minio. CloudBerry Backup - CloudBerry Backup is used to store files, folders and system images to cloud storage providers. CloudBerry uses Minio for standalone, online and managed backup service. Duplicati - Duplicati is free, open source, backup software that implements full encryption, compression, and de-duplication that fully obscures backup contents from data hosting providers. It supports S3-compatible services, allowing Minio to server as the backend storage. pgBackRest - Reliable PostgreSQL Backup & Restore. Minio can provide repository storage for pgBackrest using the built-in S3 driver. rclone - \"rsync for cloud storage\". Rclone is a command line application to sync files to and from cloud storage systems and it works well with Minio. Check out rclone's s3 docs for more information. restic - restic is a backup program that is fast, efficient and secure. Check the documentation for instructions on how to backup to a Minio server using restic. s3git - git for cloud storage. s3git provides distributed version control for data. Create decentralized and versioned repos that scale infinitely to 100s of millions of files. Clone huge PB-scale repos on your local SSD to make changes, commit and push back. Check out s3git docs for more information. Cloud Storage Clients \u00b6 Cloud Explorer - Cloud Explorer is an open source client written in Java and runs on any OS. It has many unique features such as a a text editor, performance testor, image viewer, search, bucket migrations, music player, IRC client, and much more. Cyberduck - Cyberduck is an open source client for FTP and SFTP, WebDAV, OpenStack Swift, and Amazon S3, available on Mac OS X and Windows. It supports Minio as an AWS S3 compatible storage. Filestash ( demo ) - A Dropbox-like web client that supports Minio as one of the backends. Mountain Duck - Mountain Duck lets you mount a server and cloud storage as a local disk within the Finder app on Mac and the File Explorer app on Windows. It supports Minio as an AWS S3 compatible storage. Web Application Framework \u00b6 Django-Minio - Django is a popular Python web framework. Django-Minio plugin enables use of Minio as an AWS S3 replacement. Minio-Play-Rest-API - Rest API for Minio ( AWS S3 compatible object storage server ) based on Java Play Framework 2. Minio as Managed Service \u00b6 Apcera - Apcera offers a Minio service gateway to their users. Minio enables Apcera users to spin up a persistent, S3 compatible datastore on any infrastructure with just a single command. Minio provides an open source AWS S3 alternative for Apcera users. Compatible Hardware \u00b6 Cisco UCS C240 M4 Rack Server - Cisco 2U DP E5-2600v3 platform with 12x 3.5\u201d drive bays Intel\u00ae Server System R2312WTTYSR - Intel 2U DP E5-2600v3 platform with 12x 3.5\u201d drive bays Quanta Grid D51B-2U (OCP Compliant) - Quanta 2U DP E5-2600v3 platform with 12x 3.5\u201d drive bays SMC 5018A-AR12L (Intel Atom) - SMC 1U SoC Atom C2750 platform with 12x 3.5\u201d drive bays File Sharing \u00b6 Pydio Cells - Pydio Cells is an open source sync & share platform written in Go. Cells is using Minio as an object storage backend for serving files. It is also compatible with Amazon S3, Azure and other S3 storage providers. Check out Pydio Cells repository for more information and/or to contribute.","title":"Minio Guide"},{"location":"guides/ops-services/minio-guilde/#minio-guide","text":"","title":"Minio Guide"},{"location":"guides/ops-services/minio-guilde/#rclone-vs-minio","text":"https://www.libhunt.com/compare-minio-vs-rclone?ref=compare https://forum.rclone.org/t/recommendations-for-using-rclone-with-a-minio-10m-files/14472 https://golangrepo.com/tag/terraform-provider-minio_star_1 https://www.huuphan.com/2021/08/rclone-backup-all-vps-to-microsoft.html https://awstools.dev/ https://opensourcelibs.com/libs/s3 https://strobelstefan.org/2021/03/02/rclone-dateien-von-git-repository-automatisch-zu-nexcloud-uebertragen/","title":"Rclone vs Minio"},{"location":"guides/ops-services/minio-guilde/#awesome-minio","text":"from https://github.com/minio/awesome-minio A curated list of Minio community projects inspired by awesome-go. content A curated list of Minio community projects inspired by awesome-go .","title":"Awesome Minio"},{"location":"guides/ops-services/minio-guilde/#contribution-guidelines","text":"Add entries alphabetically, under the appropriate category. To add, remove, or change things on the list: Submit a pull request. Description should contain a link with the name of the package/project/website. Do not exceed more than a paragraph.","title":"Contribution Guidelines"},{"location":"guides/ops-services/minio-guilde/#cloud-paas","text":"Cloudron - Cloudron is a platform that makes it easy to run web apps on your server and keep them up-to-date. Cloudron supports storing it's backups on Minio, you can read more here . Minio itself has been packaged as a Cloudron App - you can try it in the demo (username: cloudron password: cloudron) Minio on Cloudron App Store - Package Source . Hephy Workflow (formerly known as Deis Workflow) - an open source Platform-as-a-Service for Kubernetes . Workflow uses Minio by default to store all internal application metadata and database backups. Jelastic - Jelastic is a multi-cloud PaaS and CaaS for business. It uses Minio as an S3 compatible object storage server in Docker containers. minio-dist-boshrelease - Cloud Foundry BOSH is an open source tool chain for release engineering, deployment and lifecycle management of large scale distributed services. minio-dist-boshrelease will help easily install the distributed version of Minio . minio-dokku - Dockerfile to run Minio on Dokku PaaS. Rafter - Kubernetes-native files/assets store powered by MinIO. It extends Kubernetes with Custom Resource Definitions like AssetGroup, Asset and Bucket. It supports webhook approach that you can modify/validate/analyze a file asynchronously before it is being saved into MinIO. sloppy.io - sloppy.io is the fastest way to deploy your docker container online. We provide the infrastructure and workflow to run container applications and micro services. Access our platform over the web, via CLI or our own API. Check out Deploying Minio to sloppy.io for further info! Univention Corporate Server - Univention Corporate Server (UCS) is the innovative basis for the cost-efficient operation and easy administration of server applications and entire IT infrastructures. The integrated App Center offers a multitude of enterprise solutions, which can be run or operated virtually with just a few clicks. Minio has been packages as an app for Univention - you can try it from the Univention App Center - Source .","title":"Cloud - PaaS"},{"location":"guides/ops-services/minio-guilde/#cloud-iaas","text":"Digital Ocean - Deploy an SSD cloud server in 55 seconds. Eucalyptus - Eucalyptus is an open solution to build private and hybrid clouds that are compatible with Amazon Web Services. Minio can be used as an object storage backend for Eucalyptus. Ha-Minio - Configures Minio, as a S3 compatible cloud storage server for DigitalOcean in a highly available fashion. Onlinetech - Secure, compliant enterprise cloud. OVH - Build your own infrastructure with OVH public cloud.. Packet - Packet is a baremetal cloud provider. Plumbery - Plumbery is an open source project that was initiated by Dimension Data to accelerate digital transformation. It recommends using Minio as a standalone object storage. s3-tus-store - Minio is the supported storage backend for https://tus.io/ . Tus is an open protocol for resumable file uploads. ScaleWay - Scalable and affordable cloud and bare-metal servers (European DC). SSD Nodes - Simple, high performance cloud provider with truly personalized support.","title":"Cloud - IaaS"},{"location":"guides/ops-services/minio-guilde/#devops-cicd","text":"drone.io - drone.io is continuous integration for Github and Bitbucket, that monitors your code for bugs. It uses Minio as an AWS S3 cloud storage alternative. Gorbachev IO - Gorbachev is a continuous integration platform, used to manage reproducible research at Dragonfly Data Science. Minio tools are awesome, and the community is supportive and responsive. mkrepo - maintains an RPM or DEB repository in S3, and periodically regenerates metadata. You can use Minio as a convenient and inexpensive storage for your CI. s3-resource-simple - s3-resource-simple is Concourse CI's resource for uploading files to S3. It supports Minio as an object storage backend. puppet-minio - Puppet module to manage Minio installations. Uses the binary, does not (yet) support Docker based installation. (Requires Puppet version 4+)","title":"DevOps - CI/CD"},{"location":"guides/ops-services/minio-guilde/#cloudsaas-applications","text":"Appknox - Appknox detects & addresses vulnerabilities in mobile apps within minutes. Managing storage was a nightmare for us before Minio came along. We use Minio to store all our ipa/apk files and pdf reports that gets auto-nenerated. We are an enterprise startup and we offer on-premise and private-cloud installations of our cloud-based scanner. We have deployed our code base on AWS, Google, Azure, multiple on-premise installations and private cloud. Before Mino came along - we had to write and maintain Python libraries for each platform and maintain them. Some of the changes that we make cannot be even tested - because it is deployed on-premise which we dont have access to. Thankfully, we came across Minio. Now we just install minio everywhere and forget about storage. Minio Rocks! Blockai - Blockai helps artists claim their copyrights and protects them, for free. It uses Minio for all internal development and testing as an AWS S3 compatible object storage service. Cosmic App - Cosmic App uses Minio for storing client files and quote information for commercial brokers. Files are packaged up for lenders including high street banks to access securely. Crisp - Crisp is a Customer Intelligence Platform. It helps teams know, understand and communicate with their customers, from multiple channels (chat, email). Minio is used to securely store user file uploads. Files are uploaded from the chatbox, operator dashboard and email attachments. Minio completely replaces S3 and brings more control to user data safety. dapploy - dapploy is a solution to create private app stores to deploy iOS and Android applications to your team, company or organization. Minio is easily integrated with dapploy's architecture and docker environment. Minio is used to store application packages - ipas, apks, icons and images. Minio's great feature among others is the ability to have presigned urls while downloading resources. We use that a lot. Thanks for your good documentation and community. DBHub.io - Online storage, visualisation, and collaboration for SQLite databases. Minio is the primary object store for the SQLite databases. GitLab - GitLab is an open source software to collaborate on code. It uses Minio to store objects in their caching server. Mattermost - Mattermost is an open source, self-hosted Slack alternative. It uses minio-go fully to handle S3 API requests. Pathio - Pathio is a version control system for Excel. It uses Minio as an on-premise storage backend for workbooks and json blobs. QuezX - QuezX is a recruitment aggregator for connecting employers to recruitment agencies. It uses Minio to store and manage CV's and other documents on their platform. Trustvox - Trustvox is an integrator of ecommerce platforms, ensures 100% valid product reviews, allowing only real buyers to post reviews along with a continuous audit process. The platform team chose Minio to create its own S3 service to avoid infrastructure vendor lock-in, obtaining transparent compatibility with Google Cloud Storage and previous Amazon S3 infrastructures. We tested other options before, and only Minio showed the stable and enterprise-grade results we were looking for. Rocket.Chat - Rocket.Chat is a completely FOSS alternative to Slack. In Rocket.Chat Minio can be used for file uploads \u2014 this applies to channels, groups and privately between one user and another.","title":"Cloud/SaaS Applications"},{"location":"guides/ops-services/minio-guilde/#content-management-system","text":"Simple Simple Ads - uses Minio as a compatible object storage server replacement in our development environment. Multiple developers share a local repository of files transparently without having to change any site configuration. This makes developing with Drupal and the s3fs modules very easy. File changes are staged locally and then migrated to Amazon S3 after approval using the mc client utility.","title":"Content Management System"},{"location":"guides/ops-services/minio-guilde/#application-development-services","text":"C0D1UM - C0D1UM is a software development firm doing projects on demand and offering operating system infrastructure support. We use minio to secure all confidential data in private network. It was a first choice because major functions are compatible with AWS S3 standard functions. Collaborne - Collaborne uses Minio docker containers in development environments to simulate a full AWS S3 environment. nxsol - nxsol specializes in the development of desktop and web-based businesses and personal applications. It uses minio-java client library for Amazon S3 related applications. ToolsLib - ToolsLib provides project management services including software hosting for millions of users. Minio fits perfectly in the current infrastructure: its ability to easily scale is a crucial point for ToolsLib services.","title":"Application Development Services"},{"location":"guides/ops-services/minio-guilde/#cloud-backup-versioning","text":"Arq - Arq is a storage backend agnostic backup tool for Mac and Windows. Backend services include Amazon Cloud Drive, Google Drive, Dropbox, One Drive, Amazon S3 and more. It also supports 'Other S3-Compatible Services' which means that you can use Minio to build your own backend. BackupHive - Providing online backup services from The Netherlands with Minio as an S3 compatible back-end server to store and retrieve files. Minio is very scalable, uses almost no resources itself and is easy to maintain. The awesome team has a strong combined knowledge of use-cases, ranging from the smallest personal project to large scale cross-datacenter setups, all available within a comfortable community. burry - Burry, the BackUp & RecoveRY tool for cloud native infrastructure services enables to backup and restore ZooKeeper, etcd and Consul to and from local storage, Amazon S3, Azure Storage, Google Storage, Minio. CloudBerry Backup - CloudBerry Backup is used to store files, folders and system images to cloud storage providers. CloudBerry uses Minio for standalone, online and managed backup service. Duplicati - Duplicati is free, open source, backup software that implements full encryption, compression, and de-duplication that fully obscures backup contents from data hosting providers. It supports S3-compatible services, allowing Minio to server as the backend storage. pgBackRest - Reliable PostgreSQL Backup & Restore. Minio can provide repository storage for pgBackrest using the built-in S3 driver. rclone - \"rsync for cloud storage\". Rclone is a command line application to sync files to and from cloud storage systems and it works well with Minio. Check out rclone's s3 docs for more information. restic - restic is a backup program that is fast, efficient and secure. Check the documentation for instructions on how to backup to a Minio server using restic. s3git - git for cloud storage. s3git provides distributed version control for data. Create decentralized and versioned repos that scale infinitely to 100s of millions of files. Clone huge PB-scale repos on your local SSD to make changes, commit and push back. Check out s3git docs for more information.","title":"Cloud Backup / Versioning"},{"location":"guides/ops-services/minio-guilde/#cloud-storage-clients","text":"Cloud Explorer - Cloud Explorer is an open source client written in Java and runs on any OS. It has many unique features such as a a text editor, performance testor, image viewer, search, bucket migrations, music player, IRC client, and much more. Cyberduck - Cyberduck is an open source client for FTP and SFTP, WebDAV, OpenStack Swift, and Amazon S3, available on Mac OS X and Windows. It supports Minio as an AWS S3 compatible storage. Filestash ( demo ) - A Dropbox-like web client that supports Minio as one of the backends. Mountain Duck - Mountain Duck lets you mount a server and cloud storage as a local disk within the Finder app on Mac and the File Explorer app on Windows. It supports Minio as an AWS S3 compatible storage.","title":"Cloud Storage Clients"},{"location":"guides/ops-services/minio-guilde/#web-application-framework","text":"Django-Minio - Django is a popular Python web framework. Django-Minio plugin enables use of Minio as an AWS S3 replacement. Minio-Play-Rest-API - Rest API for Minio ( AWS S3 compatible object storage server ) based on Java Play Framework 2.","title":"Web Application Framework"},{"location":"guides/ops-services/minio-guilde/#minio-as-managed-service","text":"Apcera - Apcera offers a Minio service gateway to their users. Minio enables Apcera users to spin up a persistent, S3 compatible datastore on any infrastructure with just a single command. Minio provides an open source AWS S3 alternative for Apcera users.","title":"Minio as Managed Service"},{"location":"guides/ops-services/minio-guilde/#compatible-hardware","text":"Cisco UCS C240 M4 Rack Server - Cisco 2U DP E5-2600v3 platform with 12x 3.5\u201d drive bays Intel\u00ae Server System R2312WTTYSR - Intel 2U DP E5-2600v3 platform with 12x 3.5\u201d drive bays Quanta Grid D51B-2U (OCP Compliant) - Quanta 2U DP E5-2600v3 platform with 12x 3.5\u201d drive bays SMC 5018A-AR12L (Intel Atom) - SMC 1U SoC Atom C2750 platform with 12x 3.5\u201d drive bays","title":"Compatible Hardware"},{"location":"guides/ops-services/minio-guilde/#file-sharing","text":"Pydio Cells - Pydio Cells is an open source sync & share platform written in Go. Cells is using Minio as an object storage backend for serving files. It is also compatible with Amazon S3, Azure and other S3 storage providers. Check out Pydio Cells repository for more information and/or to contribute.","title":"File Sharing"},{"location":"guides/ops-services/vault-guide/","text":"Vault Guide \u00b6 Vault is security/auth block. That ca be use for local and cloud authentication Examples \u00b6 [TBD] Reference \u00b6 How To Use Hashicorp Vault And Argo Cd For Gitops On openshift Terraform Google Vault Gitops With Argocd And Hashicorp Vault On Kubernetes/ ArgoCD and Vault on Red Hat https://github.com/Thierry61/safe_vault_local_network https://github.com/sk3pp3r/cheat-sheet-pdf Video \u00b6 https://www.youtube.com/watch?v=7aR6k9xBN94&list=PL7bmigfV0EqQzxcNpmcdTJ9eFRPBe-iZa&index=42 https://www.suse.com/c/rancher_blog/implementing-gitops-on-kubernetes-using-k3s-rancher-vault-and-argo-cd/?hss_channel=lcp-4819449","title":"Vault Guide"},{"location":"guides/ops-services/vault-guide/#vault-guide","text":"Vault is security/auth block. That ca be use for local and cloud authentication","title":"Vault Guide"},{"location":"guides/ops-services/vault-guide/#examples","text":"[TBD]","title":"Examples"},{"location":"guides/ops-services/vault-guide/#reference","text":"How To Use Hashicorp Vault And Argo Cd For Gitops On openshift Terraform Google Vault Gitops With Argocd And Hashicorp Vault On Kubernetes/ ArgoCD and Vault on Red Hat https://github.com/Thierry61/safe_vault_local_network https://github.com/sk3pp3r/cheat-sheet-pdf","title":"Reference"},{"location":"guides/ops-services/vault-guide/#video","text":"https://www.youtube.com/watch?v=7aR6k9xBN94&list=PL7bmigfV0EqQzxcNpmcdTJ9eFRPBe-iZa&index=42 https://www.suse.com/c/rancher_blog/implementing-gitops-on-kubernetes-using-k3s-rancher-vault-and-argo-cd/?hss_channel=lcp-4819449","title":"Video"},{"location":"guides/ops-tools/base-tools-intro/","text":"Essential Tools \u00b6 Minimal toolset that we need is rlone task yq yq v4 While Os-Sys Minimal toolset docker ubuntu","title":"Essential Tools"},{"location":"guides/ops-tools/base-tools-intro/#essential-tools","text":"Minimal toolset that we need is rlone task yq yq v4 While Os-Sys Minimal toolset docker ubuntu","title":"Essential Tools"},{"location":"guides/ops-tools/bash-tests/","text":"yairdar.github.io.publisher \u00b6 pubish to repo scripts Testing bashh applications \u00b6 #!/usr/bin/expect -f set timeout -1 spawn ../4/questions.sh expect \"Your name: \" send -- \"expect\\n\" expect \"Are you human?\\r y/n: \" send -- \"n\\r\" expect \"What is your favorite programming language?\\r Your answer: \" send -- \"Java\\r\" expect eof","title":"yairdar.github.io.publisher"},{"location":"guides/ops-tools/bash-tests/#yairdargithubiopublisher","text":"pubish to repo scripts","title":"yairdar.github.io.publisher"},{"location":"guides/ops-tools/bash-tests/#testing-bashh-applications","text":"#!/usr/bin/expect -f set timeout -1 spawn ../4/questions.sh expect \"Your name: \" send -- \"expect\\n\" expect \"Are you human?\\r y/n: \" send -- \"n\\r\" expect \"What is your favorite programming language?\\r Your answer: \" send -- \"Java\\r\" expect eof","title":"Testing bashh applications"},{"location":"guides/ops-tools/jq-guide/","text":"JQ guide \u00b6 https://earthly.dev/blog/jq-select/ Background: Fingers, Head, and Google https://jqterm.com/?query= . https://github.com/rjz/jq-tutorial Basic concepts \u00b6 origin_url: https://gist.github.com/olih/f7437fb6962fb3ee9fe95bda8d2c8fa4 The syntax for jq is pretty coherent: Syntax Description , Filters separated by a comma will produce multiple independent outputs ? Will ignores error if the type is unexpected [] Array construction {} Object construction + Concatenate or Add - Difference of sets or Substract length Size of selected element | Pipes are used to chain commands in a similar fashion than bash Dealing with json objects \u00b6 Description Command Display all keys jq 'keys' Adds + 1 to all items jq 'map_values(.+1)' Delete a key jq 'del(.foo)' Convert an object to array to_entries &#124; map([.key, .value]) Dealing with fields \u00b6 Description Command Concatenate two fields fieldNew=.field1+' '+.field2 Dealing with json arrays \u00b6 Slicing and Filtering \u00b6 Description Command All jq .[] First jq '.[0]' Range jq '.[2:4]' First 3 jq '.[:3]' Last 2 jq '.[-2:]' Before Last jq '.[-2]' Select array of int by value jq 'map(select(. >= 2))' Select array of objects by value ** jq '.[] | select(.id == \"second\")'** Select by type ** jq '.[] | numbers' ** with type been arrays, objects, iterables, booleans, numbers, normals, finites, strings, nulls, values, scalars Mapping and Transforming \u00b6 Description Command Add + 1 to all items jq 'map(.+1)' Delete 2 items jq 'del(.[1, 2])' Concatenate arrays jq 'add' Flatten an array jq 'flatten' Create a range of numbers jq '[range(2;4)]' Display the type of each item jq 'map(type)' Sort an array of basic type jq 'sort' Sort an array of objects jq 'sort_by(.foo)' Group by a key - opposite to flatten jq 'group_by(.foo)' Minimun value of an array jq 'min' .See also min, max, min_by(path_exp), max_by(path_exp) Remove duplicates jq 'unique' or jq 'unique_by(.foo)' or jq 'unique_by(length)' Reverse an array jq 'reverse'","title":"JQ guide"},{"location":"guides/ops-tools/jq-guide/#jq-guide","text":"https://earthly.dev/blog/jq-select/ Background: Fingers, Head, and Google https://jqterm.com/?query= . https://github.com/rjz/jq-tutorial","title":"JQ guide"},{"location":"guides/ops-tools/jq-guide/#basic-concepts","text":"origin_url: https://gist.github.com/olih/f7437fb6962fb3ee9fe95bda8d2c8fa4 The syntax for jq is pretty coherent: Syntax Description , Filters separated by a comma will produce multiple independent outputs ? Will ignores error if the type is unexpected [] Array construction {} Object construction + Concatenate or Add - Difference of sets or Substract length Size of selected element | Pipes are used to chain commands in a similar fashion than bash","title":"Basic concepts"},{"location":"guides/ops-tools/jq-guide/#dealing-with-json-objects","text":"Description Command Display all keys jq 'keys' Adds + 1 to all items jq 'map_values(.+1)' Delete a key jq 'del(.foo)' Convert an object to array to_entries &#124; map([.key, .value])","title":"Dealing with json objects"},{"location":"guides/ops-tools/jq-guide/#dealing-with-fields","text":"Description Command Concatenate two fields fieldNew=.field1+' '+.field2","title":"Dealing with fields"},{"location":"guides/ops-tools/jq-guide/#dealing-with-json-arrays","text":"","title":"Dealing with json arrays"},{"location":"guides/ops-tools/jq-guide/#slicing-and-filtering","text":"Description Command All jq .[] First jq '.[0]' Range jq '.[2:4]' First 3 jq '.[:3]' Last 2 jq '.[-2:]' Before Last jq '.[-2]' Select array of int by value jq 'map(select(. >= 2))' Select array of objects by value ** jq '.[] | select(.id == \"second\")'** Select by type ** jq '.[] | numbers' ** with type been arrays, objects, iterables, booleans, numbers, normals, finites, strings, nulls, values, scalars","title":"Slicing and Filtering"},{"location":"guides/ops-tools/jq-guide/#mapping-and-transforming","text":"Description Command Add + 1 to all items jq 'map(.+1)' Delete 2 items jq 'del(.[1, 2])' Concatenate arrays jq 'add' Flatten an array jq 'flatten' Create a range of numbers jq '[range(2;4)]' Display the type of each item jq 'map(type)' Sort an array of basic type jq 'sort' Sort an array of objects jq 'sort_by(.foo)' Group by a key - opposite to flatten jq 'group_by(.foo)' Minimun value of an array jq 'min' .See also min, max, min_by(path_exp), max_by(path_exp) Remove duplicates jq 'unique' or jq 'unique_by(.foo)' or jq 'unique_by(length)' Reverse an array jq 'reverse'","title":"Mapping and Transforming"},{"location":"guides/ops-tools/mkdocs-guide/","text":"Mkdocs Guide \u00b6 Templates \u00b6 Lowhanging Fruite tutorial codeinside Tutorial Gnu Linux MkDocs Minimal Start \u00b6 Mermaid \u00b6 FULL Picture graph LR A --> B Plugins \u00b6 mkdocs-literate-nav awesome pages Items \u00b6 Diagrams \u00b6 https://lowhangingfruit.gitlab.io/mkdocstemplate/maintainer-info/mermaid/ Navigations \u00b6 https://squidfunk.github.io/mkdocs-material/setup/setting-up-navigation/ Tables \u00b6 Tables To get this navigation, create the file SUMMARY.md : (old YAML equivalent:) * [Frob](#index.md) * [Baz](#baz.md) * Borgs * [Bar](#borgs/bar.md) * [Foo](#borgs/foo.md) * [ Frob ]( index.md ) * [ Baz ]( baz.md ) * Borgs * [ Bar ]( borgs/bar.md ) * [ Foo ]( borgs/foo.md ) nav : - Frob : index.md - Baz : baz.md - Borgs : - Bar : borgs/bar.md - Foo : borgs/foo.md Test Admonitions \u00b6 mkdocs-guide content Test Mermaid Graph \u00b6 graph LR A --> B Test Tabbed Content \u00b6 TAB 1 list with items TAB B some par","title":"Mkdocs Guide"},{"location":"guides/ops-tools/mkdocs-guide/#mkdocs-guide","text":"","title":"Mkdocs Guide"},{"location":"guides/ops-tools/mkdocs-guide/#templates","text":"Lowhanging Fruite tutorial codeinside Tutorial Gnu Linux MkDocs","title":"Templates"},{"location":"guides/ops-tools/mkdocs-guide/#minimal-start","text":"","title":"Minimal Start"},{"location":"guides/ops-tools/mkdocs-guide/#mermaid","text":"FULL Picture graph LR A --> B","title":"Mermaid"},{"location":"guides/ops-tools/mkdocs-guide/#plugins","text":"mkdocs-literate-nav awesome pages","title":"Plugins"},{"location":"guides/ops-tools/mkdocs-guide/#items","text":"","title":"Items"},{"location":"guides/ops-tools/mkdocs-guide/#diagrams","text":"https://lowhangingfruit.gitlab.io/mkdocstemplate/maintainer-info/mermaid/","title":"Diagrams"},{"location":"guides/ops-tools/mkdocs-guide/#navigations","text":"https://squidfunk.github.io/mkdocs-material/setup/setting-up-navigation/","title":"Navigations"},{"location":"guides/ops-tools/mkdocs-guide/#tables","text":"Tables To get this navigation, create the file SUMMARY.md : (old YAML equivalent:) * [Frob](#index.md) * [Baz](#baz.md) * Borgs * [Bar](#borgs/bar.md) * [Foo](#borgs/foo.md) * [ Frob ]( index.md ) * [ Baz ]( baz.md ) * Borgs * [ Bar ]( borgs/bar.md ) * [ Foo ]( borgs/foo.md ) nav : - Frob : index.md - Baz : baz.md - Borgs : - Bar : borgs/bar.md - Foo : borgs/foo.md","title":"Tables"},{"location":"guides/ops-tools/mkdocs-guide/#test-admonitions","text":"mkdocs-guide content","title":"Test Admonitions"},{"location":"guides/ops-tools/mkdocs-guide/#test-mermaid-graph","text":"graph LR A --> B","title":"Test Mermaid Graph"},{"location":"guides/ops-tools/mkdocs-guide/#test-tabbed-content","text":"TAB 1 list with items TAB B some par","title":"Test Tabbed Content"},{"location":"guides/ops-tools/shell-guide/","text":"Shell Guide \u00b6 Amazing Shell Tips \u00b6 https://www.shell-tips.com/bash/loops/ Interactive Shell \u00b6 https://www.baeldung.com/linux/bash-interactive-prompts oh my zsh \u00b6 zsh:oh-my-zsh:plugins docker-compose Zsh Completions \u00b6 https://scriptingosx.com/2019/07/moving-to-zsh-part-5-completions/ https://zsh.sourceforge.io/Doc/Release/Completion-System.html Bash Script Parts \u00b6 Following next naming convention Info layout : |- ### TOPIC. CHAPTER_NUM__NO_ZEROS. CHAPTER_NAME terms : TOPIC : name id of series. here='Bash Script Parts' CHAPTER_NUM__NO_ZEROS : |- desc: chapter number in order of entry in script or complexity no zeros: to elimitate zero interpretation issues no zeros allowed CHAPTER_NAME : name of the chapter sample='Shebang Line' Bash Script. P11. Shebang Line \u00b6 How does /usr/bin/env work in a Linux shebang line? !!! question What is shebang > First line in bash / sh scripts usually looks like ```bash #!/usr/bin/env python3 ``` !!! question How does /usr/bin/env work in a Linux shebang line? > `_origin_link_` [stack-overflow-question](https://stackoverflow.com/questions/43793040/how-does-usr-bin-env-work-in-a-linux-shebang-line) `env` is the name of a Unix program. If you read the manual (`man env`) you can see that one way to use it is `env COMMAND`, where in your case, `COMMAND` is `python3`. According to the manual, this will > Set each `NAME` to `VALUE` in the environment and run `COMMAND`. Running env alone will show you what NAMEs and VALUEs are set: ```sh $ env TERM=xterm-256color SHELL=/bin/bash PATH=/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/usr/sbin:/sbin ``` Therefore, `/usr/bin/env python3` is an instruction to set the `PATH` (as well as all the other `NAME+VALUE` pairs), and then run `python3`, using the first directory in the `PATH` that contains the `python3` executable. !!! tip Reference - How to use the /usr/bin/env command in a shell script - Why is it better to use \"#!/usr/bin/env NAME\" instead of \"#!/path/to/NAME\" as my shebang? Sdf sdf sdf sdf","title":"Shell Guide"},{"location":"guides/ops-tools/shell-guide/#shell-guide","text":"","title":"Shell Guide"},{"location":"guides/ops-tools/shell-guide/#amazing-shell-tips","text":"https://www.shell-tips.com/bash/loops/","title":"Amazing Shell Tips"},{"location":"guides/ops-tools/shell-guide/#interactive-shell","text":"https://www.baeldung.com/linux/bash-interactive-prompts","title":"Interactive Shell"},{"location":"guides/ops-tools/shell-guide/#oh-my-zsh","text":"zsh:oh-my-zsh:plugins docker-compose","title":"oh my zsh"},{"location":"guides/ops-tools/shell-guide/#zsh-completions","text":"https://scriptingosx.com/2019/07/moving-to-zsh-part-5-completions/ https://zsh.sourceforge.io/Doc/Release/Completion-System.html","title":"Zsh Completions"},{"location":"guides/ops-tools/shell-guide/#bash-script-parts","text":"Following next naming convention Info layout : |- ### TOPIC. CHAPTER_NUM__NO_ZEROS. CHAPTER_NAME terms : TOPIC : name id of series. here='Bash Script Parts' CHAPTER_NUM__NO_ZEROS : |- desc: chapter number in order of entry in script or complexity no zeros: to elimitate zero interpretation issues no zeros allowed CHAPTER_NAME : name of the chapter sample='Shebang Line'","title":"Bash Script Parts"},{"location":"guides/ops-tools/shell-guide/#bash-script-p11-shebang-line","text":"How does /usr/bin/env work in a Linux shebang line? !!! question What is shebang > First line in bash / sh scripts usually looks like ```bash #!/usr/bin/env python3 ``` !!! question How does /usr/bin/env work in a Linux shebang line? > `_origin_link_` [stack-overflow-question](https://stackoverflow.com/questions/43793040/how-does-usr-bin-env-work-in-a-linux-shebang-line) `env` is the name of a Unix program. If you read the manual (`man env`) you can see that one way to use it is `env COMMAND`, where in your case, `COMMAND` is `python3`. According to the manual, this will > Set each `NAME` to `VALUE` in the environment and run `COMMAND`. Running env alone will show you what NAMEs and VALUEs are set: ```sh $ env TERM=xterm-256color SHELL=/bin/bash PATH=/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/usr/sbin:/sbin ``` Therefore, `/usr/bin/env python3` is an instruction to set the `PATH` (as well as all the other `NAME+VALUE` pairs), and then run `python3`, using the first directory in the `PATH` that contains the `python3` executable. !!! tip Reference - How to use the /usr/bin/env command in a shell script - Why is it better to use \"#!/usr/bin/env NAME\" instead of \"#!/path/to/NAME\" as my shebang? Sdf sdf sdf sdf","title":"Bash Script. P11. Shebang Line"},{"location":"guides/ops-tools/taskgo-guide/","text":"Taskgo guide \u00b6 https://conemu.github.io/en/wsl.html#start https://dev.to/stack-labs/introduction-to-taskfile-a-makefile-alternative-h92","title":"Taskgo guide"},{"location":"guides/ops-tools/taskgo-guide/#taskgo-guide","text":"https://conemu.github.io/en/wsl.html#start https://dev.to/stack-labs/introduction-to-taskfile-a-makefile-alternative-h92","title":"Taskgo guide"},{"location":"guides/ops-tools/yaml-guide/","text":"Yaml Guide \u00b6 Please check https://yt-project.org/ https://yt-project.org/ Templating Options \u00b6 ytt tool docs ytt tool tut-101 Executors \u00b6 taskgo","title":"Yaml Guide"},{"location":"guides/ops-tools/yaml-guide/#yaml-guide","text":"Please check https://yt-project.org/ https://yt-project.org/","title":"Yaml Guide"},{"location":"guides/ops-tools/yaml-guide/#templating-options","text":"ytt tool docs ytt tool tut-101","title":"Templating Options"},{"location":"guides/ops-tools/yaml-guide/#executors","text":"taskgo","title":"Executors"},{"location":"guides/ops-tools/date-time/cron-guide/","text":"Cron Guide \u00b6 Cron CheatSheet \u00b6","title":"Cron Guide"},{"location":"guides/ops-tools/date-time/cron-guide/#cron-guide","text":"","title":"Cron Guide"},{"location":"guides/ops-tools/date-time/cron-guide/#cron-cheatsheet","text":"","title":"Cron CheatSheet"},{"location":"yd-contacts/contacts/","text":"Contacts \u00b6 Yair Dar LinkedIn Personal Web Site GitHub YairDar Tech Website YairDev LinkedIn \u00b6 Github \u00b6","title":"Contacts"},{"location":"yd-contacts/contacts/#contacts","text":"Yair Dar LinkedIn Personal Web Site GitHub YairDar Tech Website YairDev","title":"Contacts"},{"location":"yd-contacts/contacts/#linkedin","text":"","title":"LinkedIn"},{"location":"yd-contacts/contacts/#github","text":"","title":"Github"}]}